{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "# ! pip install fastai nbdev sentencepiece\n",
    "# ! pip install fastai --upgrade\n",
    "# ! pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import sentencepiece as sp\n",
    "from fastai.text import *\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import json\n",
    "import pandas as pd\n",
    "warnings.simplefilter(\"ignore\", UserWarning)  #no UserWarning display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Dataframe Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_file = \"Data1/data_round_1/train/train.token.code\" \n",
    "nl_file = \"Data1/data_round_1/train/train.token.nl\"\n",
    "ast_file = \"Data1/data_round_1/train/train.token.ast\" \n",
    "\n",
    "with open(code_file, \"r\", encoding=\"utf-8\") as f_code, open(nl_file, \"r\", encoding=\"utf-8\") as f_nl, open(ast_file, \"r\", encoding=\"utf-8\") as f_ast:\n",
    "    code_lines = f_code.readlines()\n",
    "    nl_lines = f_nl.readlines()\n",
    "    ast_lines = f_ast.readlines()\n",
    "\n",
    "assert len(code_lines) == len(nl_lines), \"Mismatch in the number of lines between code and nl files\"\n",
    "df_trn = pd.DataFrame({\"code\": [line.strip() for line in code_lines], \n",
    "                   \"comment\": [line.strip() for line in nl_lines],\n",
    "                    \"ast\": [line.strip() for line in ast_lines]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "comment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ast",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "75c7104c-243d-46d5-b598-aefbfac398c7",
       "rows": [
        [
         "0",
         "private Environment . Frame createGlobals ( EventHandler eventHandler , Options options , ConfigFile configFile ) { Environment env = createEnvironment ( eventHandler , Environment . SKYLARK , ImmutableMap . < String , Extension > of ( ) ) ; for ( Class < ? > module : modules ) { logger . log ( Level . INFO , STR_ + module . getName ( ) ) ; Runtime . registerModuleGlobals ( env , module ) ; if ( OptionsAwareModule . class . isAssignableFrom ( module ) ) { ( ( OptionsAwareModule ) getModuleGlobal ( env , module ) ) . setOptions ( options ) ; } if ( LabelsAwareModule . class . isAssignableFrom ( module ) ) { ( ( LabelsAwareModule ) getModuleGlobal ( env , module ) ) . setConfigFile ( configFile ) ; } } env . mutability ( ) . close ( ) ; return env . getGlobals ( ) ; }",
         "create native global variables from the modules the returned object can be reused for different instances of environments .",
         "( MethodDeclaration ( ReferenceType ( ReferenceType ) ReferenceType ) ReferenceType ( FormalParameter ( ReferenceType ) ReferenceType ) FormalParameter ( FormalParameter ( ReferenceType ) ReferenceType ) FormalParameter ( FormalParameter ( ReferenceType ) ReferenceType ) FormalParameter ( LocalVariableDeclaration ( ReferenceType ) ReferenceType ( VariableDeclarator ( MethodInvocation ( MemberReference ) MemberReference ( MemberReference ) MemberReference ( MethodInvocation ( TypeArgument ( ReferenceType ) ReferenceType ) TypeArgument ( TypeArgument ( ReferenceType ) ReferenceType ) TypeArgument ) MethodInvocation ) MethodInvocation ) VariableDeclarator ) LocalVariableDeclaration ( ForStatement ( EnhancedForControl ( VariableDeclaration ( ReferenceType ( TypeArgument ) TypeArgument ) ReferenceType ( VariableDeclarator ) VariableDeclarator ) VariableDeclaration ( MemberReference ) MemberReference ) EnhancedForControl ( BlockStatement ( StatementExpression ( MethodInvocation ( MemberReference ) MemberReference ( BinaryOperation ( MemberReference ) MemberReference ( MethodInvocation ) MethodInvocation ) BinaryOperation ) MethodInvocation ) StatementExpression ( StatementExpression ( MethodInvocation ( MemberReference ) MemberReference ( MemberReference ) MemberReference ) MethodInvocation ) StatementExpression ( IfStatement ( ClassReference ( MethodInvocation ( MemberReference ) MemberReference ) MethodInvocation ( ReferenceType ) ReferenceType ) ClassReference ( BlockStatement ( StatementExpression ( Cast ( ReferenceType ) ReferenceType ( MethodInvocation ( MemberReference ) MemberReference ( MemberReference ) MemberReference ) MethodInvocation ) Cast ) StatementExpression ) BlockStatement ) IfStatement ( IfStatement ( ClassReference ( MethodInvocation ( MemberReference ) MemberReference ) MethodInvocation ( ReferenceType ) ReferenceType ) ClassReference ( BlockStatement ( StatementExpression ( Cast ( ReferenceType ) ReferenceType ( MethodInvocation ( MemberReference ) MemberReference ( MemberReference ) MemberReference ) MethodInvocation ) Cast ) StatementExpression ) BlockStatement ) IfStatement ) BlockStatement ) ForStatement ( StatementExpression ( MethodInvocation ( MethodInvocation ) MethodInvocation ) MethodInvocation ) StatementExpression ( ReturnStatement ( MethodInvocation ) MethodInvocation ) ReturnStatement ) MethodDeclaration"
        ],
        [
         "1",
         "private boolean boundsSafe ( int x , int y ) { if ( ( y < NUM_ ) || ( y >= height ) || ( x < NUM_ ) || ( x >= width ) ) { return BOOL_ ; } return BOOL_ ; }",
         "just a simple check to see if the x y pair actually fits into the pixel array .",
         "( MethodDeclaration ( BasicType ) BasicType ( FormalParameter ( BasicType ) BasicType ) FormalParameter ( FormalParameter ( BasicType ) BasicType ) FormalParameter ( IfStatement ( BinaryOperation ( BinaryOperation ( BinaryOperation ( BinaryOperation ( MemberReference ) MemberReference ( MemberReference ) MemberReference ) BinaryOperation ( BinaryOperation ( MemberReference ) MemberReference ( MemberReference ) MemberReference ) BinaryOperation ) BinaryOperation ( BinaryOperation ( MemberReference ) MemberReference ( MemberReference ) MemberReference ) BinaryOperation ) BinaryOperation ( BinaryOperation ( MemberReference ) MemberReference ( MemberReference ) MemberReference ) BinaryOperation ) BinaryOperation ( BlockStatement ( ReturnStatement ( MemberReference ) MemberReference ) ReturnStatement ) BlockStatement ) IfStatement ( ReturnStatement ( MemberReference ) MemberReference ) ReturnStatement ) MethodDeclaration"
        ],
        [
         "2",
         "private ResourceResponse patchResourceById ( Context context , Request request , String resourceId , String revision , List < PatchOperation > patchOperations ) throws ResourceException { idRequired ( request . getResourcePath ( ) ) ; noSubObjects ( request . getResourcePath ( ) ) ; ResourceResponse resource = readResource ( context , repoId ( resourceId ) ) ; return patchResource ( context , request , resource , revision , patchOperations ) ; }",
         "patches the given resource and will also remove private properties if it is an external call based upon context .",
         "( MethodDeclaration ( ReferenceType ) ReferenceType ( FormalParameter ( ReferenceType ) ReferenceType ) FormalParameter ( FormalParameter ( ReferenceType ) ReferenceType ) FormalParameter ( FormalParameter ( ReferenceType ) ReferenceType ) FormalParameter ( FormalParameter ( ReferenceType ) ReferenceType ) FormalParameter ( FormalParameter ( ReferenceType ( TypeArgument ( ReferenceType ) ReferenceType ) TypeArgument ) ReferenceType ) FormalParameter ( StatementExpression ( MethodInvocation ( MethodInvocation ) MethodInvocation ) MethodInvocation ) StatementExpression ( StatementExpression ( MethodInvocation ( MethodInvocation ) MethodInvocation ) MethodInvocation ) StatementExpression ( LocalVariableDeclaration ( ReferenceType ) ReferenceType ( VariableDeclarator ( MethodInvocation ( MemberReference ) MemberReference ( MethodInvocation ( MemberReference ) MemberReference ) MethodInvocation ) MethodInvocation ) VariableDeclarator ) LocalVariableDeclaration ( ReturnStatement ( MethodInvocation ( MemberReference ) MemberReference ( MemberReference ) MemberReference ( MemberReference ) MemberReference ( MemberReference ) MemberReference ( MemberReference ) MemberReference ) MethodInvocation ) ReturnStatement ) MethodDeclaration"
        ],
        [
         "3",
         "private void zoomOut ( ) { chartView . zoomOut ( ) ; zoomControls . setIsZoomInEnabled ( chartView . canZoomIn ( ) ) ; zoomControls . setIsZoomOutEnabled ( chartView . canZoomOut ( ) ) ; }",
         "to zoom out .",
         "( MethodDeclaration ( StatementExpression ( MethodInvocation ) MethodInvocation ) StatementExpression ( StatementExpression ( MethodInvocation ( MethodInvocation ) MethodInvocation ) MethodInvocation ) StatementExpression ( StatementExpression ( MethodInvocation ( MethodInvocation ) MethodInvocation ) MethodInvocation ) StatementExpression ) MethodDeclaration"
        ],
        [
         "4",
         "public CombinedRangeXYPlot ( ValueAxis rangeAxis ) { super ( null , null , rangeAxis , null ) ; this . subplots = new java . util . ArrayList < XYPlot > ( ) ; }",
         "creates a new plot .",
         "( ConstructorDeclaration ( FormalParameter ( ReferenceType ) ReferenceType ) FormalParameter ( StatementExpression ( SuperConstructorInvocation ( Literal ) Literal ( Literal ) Literal ( MemberReference ) MemberReference ( Literal ) Literal ) SuperConstructorInvocation ) StatementExpression ( StatementExpression ( Assignment ( This ( MemberReference ) MemberReference ) This ( ClassCreator ( ReferenceType ( ReferenceType ( ReferenceType ( TypeArgument ( ReferenceType ) ReferenceType ) TypeArgument ) ReferenceType ) ReferenceType ) ReferenceType ) ClassCreator ) Assignment ) StatementExpression ) ConstructorDeclaration"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>comment</th>\n",
       "      <th>ast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>private Environment . Frame createGlobals ( Ev...</td>\n",
       "      <td>create native global variables from the module...</td>\n",
       "      <td>( MethodDeclaration ( ReferenceType ( Referenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>private boolean boundsSafe ( int x , int y ) {...</td>\n",
       "      <td>just a simple check to see if the x y pair act...</td>\n",
       "      <td>( MethodDeclaration ( BasicType ) BasicType ( ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>private ResourceResponse patchResourceById ( C...</td>\n",
       "      <td>patches the given resource and will also remov...</td>\n",
       "      <td>( MethodDeclaration ( ReferenceType ) Referenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>private void zoomOut ( ) { chartView . zoomOut...</td>\n",
       "      <td>to zoom out .</td>\n",
       "      <td>( MethodDeclaration ( StatementExpression ( Me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>public CombinedRangeXYPlot ( ValueAxis rangeAx...</td>\n",
       "      <td>creates a new plot .</td>\n",
       "      <td>( ConstructorDeclaration ( FormalParameter ( R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  \\\n",
       "0  private Environment . Frame createGlobals ( Ev...   \n",
       "1  private boolean boundsSafe ( int x , int y ) {...   \n",
       "2  private ResourceResponse patchResourceById ( C...   \n",
       "3  private void zoomOut ( ) { chartView . zoomOut...   \n",
       "4  public CombinedRangeXYPlot ( ValueAxis rangeAx...   \n",
       "\n",
       "                                             comment  \\\n",
       "0  create native global variables from the module...   \n",
       "1  just a simple check to see if the x y pair act...   \n",
       "2  patches the given resource and will also remov...   \n",
       "3                                      to zoom out .   \n",
       "4                               creates a new plot .   \n",
       "\n",
       "                                                 ast  \n",
       "0  ( MethodDeclaration ( ReferenceType ( Referenc...  \n",
       "1  ( MethodDeclaration ( BasicType ) BasicType ( ...  \n",
       "2  ( MethodDeclaration ( ReferenceType ) Referenc...  \n",
       "3  ( MethodDeclaration ( StatementExpression ( Me...  \n",
       "4  ( ConstructorDeclaration ( FormalParameter ( R...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test and Valid Dataframe loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_code_file = \"Data1/data_round_1/test/test.token.code\"  # Update the path\n",
    "test_nl_file = \"Data1/data_round_1/test/test.token.nl\"      # Update the path\n",
    "\n",
    "with open(test_code_file, \"r\", encoding=\"utf-8\") as f_code, open(test_nl_file, \"r\", encoding=\"utf-8\") as f_nl:\n",
    "    test_code_lines = f_code.readlines()\n",
    "    test_nl_lines = f_nl.readlines()\n",
    "\n",
    "# assert len(code_lines) == len(nl_lines), \"Mismatch in the number of lines between code and nl files\"\n",
    "df_tst = pd.DataFrame({\"code\": [line.strip() for line in test_code_lines], \n",
    "                   \"comment\": [line.strip() for line in test_nl_lines]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "comment",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "8170baff-8e1d-4f23-bc86-797e51284062",
       "rows": [
        [
         "0",
         "public int entrySize ( Object key , Object value ) throws IllegalArgumentException { if ( value == Token . TOMBSTONE ) { return NUM_ ; } int size = HeapLRUCapacityController . this . getPerEntryOverhead ( ) ; size += sizeof ( key ) ; size += sizeof ( value ) ; return size ; }",
         "as far as we re concerned all entries have the same size"
        ],
        [
         "1",
         "private void returnData ( Object ret ) { if ( myHost != null ) { myHost . returnData ( ret ) ; } }",
         "used to communicate a return object from a plugin tool to the main whitebox user interface ."
        ],
        [
         "2",
         "public CModuleFilterFieldMenu ( final JTextField filterField ) { add ( new CFilterViewsAction ( filterField ) ) ; }",
         "creates a new menu object ."
        ],
        [
         "3",
         "private void returnData ( Object ret ) { if ( myHost != null ) { myHost . returnData ( ret ) ; } }",
         "used to communicate a return object from a plugin tool to the main whitebox user interface ."
        ],
        [
         "4",
         "public TLongArrayList ( ) { this ( DEFAULT_CAPACITY ) ; }",
         "creates a new tlongarraylist instance with the default capacity ."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>public int entrySize ( Object key , Object val...</td>\n",
       "      <td>as far as we re concerned all entries have the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>private void returnData ( Object ret ) { if ( ...</td>\n",
       "      <td>used to communicate a return object from a plu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>public CModuleFilterFieldMenu ( final JTextFie...</td>\n",
       "      <td>creates a new menu object .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>private void returnData ( Object ret ) { if ( ...</td>\n",
       "      <td>used to communicate a return object from a plu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>public TLongArrayList ( ) { this ( DEFAULT_CAP...</td>\n",
       "      <td>creates a new tlongarraylist instance with the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  \\\n",
       "0  public int entrySize ( Object key , Object val...   \n",
       "1  private void returnData ( Object ret ) { if ( ...   \n",
       "2  public CModuleFilterFieldMenu ( final JTextFie...   \n",
       "3  private void returnData ( Object ret ) { if ( ...   \n",
       "4  public TLongArrayList ( ) { this ( DEFAULT_CAP...   \n",
       "\n",
       "                                             comment  \n",
       "0  as far as we re concerned all entries have the...  \n",
       "1  used to communicate a return object from a plu...  \n",
       "2                        creates a new menu object .  \n",
       "3  used to communicate a return object from a plu...  \n",
       "4  creates a new tlongarraylist instance with the...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_code_file = \"Data1/data_round_1/valid/valid.token.code\"  # Update the path\n",
    "val_nl_file = \"Data1/data_round_1/valid/valid.token.nl\"      # Update the path\n",
    "\n",
    "with open(val_code_file, \"r\", encoding=\"utf-8\") as f_code, open(val_nl_file, \"r\", encoding=\"utf-8\") as f_nl:\n",
    "    val_code_lines = f_code.readlines()\n",
    "    val_nl_lines = f_nl.readlines()\n",
    "\n",
    "# assert len(code_lines) == len(nl_lines), \"Mismatch in the number of lines between code and nl files\"\n",
    "df_val = pd.DataFrame({\"code\": [line.strip() for line in val_code_lines], \n",
    "                   \"comment\": [line.strip() for line in val_nl_lines]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "comment",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "1fc7464b-16be-4140-b564-08c176cdef28",
       "rows": [
        [
         "0",
         "public void makeCurrent ( EGLSurface eglSurface ) { if ( eGLDisplay == EGL14 . EGL_NO_DISPLAY ) { Log . d ( TAG , STR_ ) ; } if ( ! EGL14 . eglMakeCurrent ( eGLDisplay , eglSurface , eglSurface , eGLContext ) ) { throw new RuntimeException ( STR_ ) ; } }",
         "makes our egl context current using the supplied surface for both draw and read ."
        ],
        [
         "1",
         "public BigFractionFormat ( ) { }",
         "create an improper formatting instance with the default number format for the numerator and denominator ."
        ],
        [
         "2",
         "public static void main ( final String [ ] args ) { DOMTestCase . doMain ( hc_elementgettagname . class , args ) ; }",
         "runs this test from the command line ."
        ],
        [
         "3",
         "@ DSGenerator ( tool_name = STR_ , tool_version = STR_ , generated_on = STR_ , hash_original_method = STR_ , hash_generated_method = STR_ ) public boolean isPurgingAllowed ( ) { return mAllowPurging ; }",
         "is memory file purging enabled ?"
        ],
        [
         "4",
         "public JoinAppGroupDialog ( final Activity activity ) { super ( activity , DEFAULT_REQUEST_CODE ) ; }",
         "constructs a joinappgroupdialog ."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>public void makeCurrent ( EGLSurface eglSurfac...</td>\n",
       "      <td>makes our egl context current using the suppli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>public BigFractionFormat ( ) { }</td>\n",
       "      <td>create an improper formatting instance with th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>public static void main ( final String [ ] arg...</td>\n",
       "      <td>runs this test from the command line .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@ DSGenerator ( tool_name = STR_ , tool_versio...</td>\n",
       "      <td>is memory file purging enabled ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>public JoinAppGroupDialog ( final Activity act...</td>\n",
       "      <td>constructs a joinappgroupdialog .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  \\\n",
       "0  public void makeCurrent ( EGLSurface eglSurfac...   \n",
       "1                   public BigFractionFormat ( ) { }   \n",
       "2  public static void main ( final String [ ] arg...   \n",
       "3  @ DSGenerator ( tool_name = STR_ , tool_versio...   \n",
       "4  public JoinAppGroupDialog ( final Activity act...   \n",
       "\n",
       "                                             comment  \n",
       "0  makes our egl context current using the suppli...  \n",
       "1  create an improper formatting instance with th...  \n",
       "2             runs this test from the command line .  \n",
       "3                   is memory file purging enabled ?  \n",
       "4                  constructs a joinappgroupdialog .  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isASCII(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = df_trn[df_trn['comment'].apply(lambda x: isASCII(x))]\n",
    "df_val = df_val[df_val['comment'].apply(lambda x: isASCII(x))]\n",
    "df_tst = df_tst[df_tst['comment'].apply(lambda x: isASCII(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_jdocs(df):\n",
    "#     methods = []\n",
    "#     comments = []\n",
    "#     for i, row in progress_bar(list(df.iterrows())):\n",
    "#         comment = row[\"comment\"]\n",
    "#         # Remove {} text in comments from https://stackoverflow.com/questions/14596884/remove-text-between-and-in-python/14598135\n",
    "#         comment = re.sub(\"([\\{\\[]).*?([\\)\\}])\", '', comment)\n",
    "#         cleaned = []\n",
    "#         for line in comment.split('\\n'):\n",
    "#             if \"@\" in line: break\n",
    "#             cleaned.append(line)\n",
    "#         comments.append('\\n'.join(cleaned))\n",
    "#         methods.append(row[\"code\"])\n",
    "#     new_df = pd.DataFrame(zip(methods, comments), columns = [\"code\", \"comment\"])\n",
    "    # return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(445812, 20000, 20000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_trn), len(df_val), len(df_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train  a tokenizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_text_file = \"code.txt\"\n",
    "with open(r\"Data1/data_round_1/train/train.token.code\", \"r\", encoding=\"utf-8\") as code_file, \\\n",
    "     open(merged_text_file, \"w\", encoding=\"utf-8\") as merged_file:\n",
    "    for code in code_file:\n",
    "        merged_file.write(code.strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_text_file = \"comments.txt\"\n",
    "with open(r\"Data1/data_round_1/train/train.token.nl\", \"r\", encoding=\"utf-8\") as nl_file, \\\n",
    "     open(merged_text_file, \"w\", encoding=\"utf-8\") as merged_file:\n",
    "    for nl in nl_file:\n",
    "        merged_file.write(nl.strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_text_file = \"asts.txt\"\n",
    "with open(r\"Data1/data_round_1/train/train.token.ast\", \"r\", encoding=\"utf-8\") as ast_file, \\\n",
    "     open(merged_text_file, \"w\", encoding=\"utf-8\") as merged_file:\n",
    "    for ast in ast_file:\n",
    "        merged_file.write(ast.strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"code.txt\",  # Input file\n",
    "    model_prefix=\"method_tokenizer\",  # Output file prefix (creates tokenizer.model & tokenizer.vocab)\n",
    "    vocab_size=32000,  # Adjust as needed\n",
    "    character_coverage=1.0,  # Covers all characters\n",
    "    model_type=\"unigram\"  # Other options: bpe, char, word\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"comments.txt\",  # Input file\n",
    "    model_prefix=\"comment_tokenizer\",  # Output file prefix (creates tokenizer.model & tokenizer.vocab)\n",
    "    vocab_size=26000,  # Adjust as needed\n",
    "    character_coverage=1.0,  # Covers all characters\n",
    "    model_type=\"unigram\"  # Other options: bpe, char, word\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"asts.txt\",  # Input file\n",
    "    model_prefix=\"ast_tokenizer\",  # Output file prefix (creates tokenizer.model & tokenizer.vocab)\n",
    "    vocab_size=26000,  # Adjust as needed\n",
    "    character_coverage=1.0,  # Covers all characters\n",
    "    model_type=\"unigram\"  # Other options: bpe, char, word\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_collate(samples, pad_idx=1, pad_first=True, backwards=False):\n",
    "    \"Function that collect samples and adds padding. Flips token order if needed\"\n",
    "    samples = to_data(samples)\n",
    "    max_len_x,max_len_y = max([len(s[0]) for s in samples]),max([len(s[1]) for s in samples])\n",
    "    res_x = torch.zeros(len(samples), max_len_x).long() + pad_idx\n",
    "    res_y = torch.zeros(len(samples), max_len_y).long() + pad_idx\n",
    "    if backwards: pad_first = not pad_first\n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: \n",
    "            res_x[i,-len(s[0]):],res_y[i,-len(s[1]):] = LongTensor(s[0]),LongTensor(s[1])\n",
    "        else:         \n",
    "            res_x[i, :len(s[0])],res_y[i, :len(s[1])] = LongTensor(s[0]),LongTensor(s[1])\n",
    "    if backwards: res_x,res_y = res_x.flip(1),res_y.flip(1)\n",
    "    return res_x, res_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #collapse_show\n",
    "# def seq2seq_collate(samples, pad_idx=1):\n",
    "#     \"\"\"Custom collate function for padding sequences.\"\"\"\n",
    "#     if len(samples) == 0: return None\n",
    "#     x, y = zip(*samples)\n",
    "#     x_pad = pad_sequence([tensor(o) for o in x], padding_value=pad_idx, batch_first=True)\n",
    "#     y_pad = pad_sequence([tensor(o) for o in y], padding_value=pad_idx, batch_first=True)\n",
    "#     return x_pad, y_pad\n",
    "\n",
    "# class Seq2SeqDataBunch(TextDataBunch):\n",
    "#     \"Create a `TextDataBunch` suitable for training an RNN classifier.\"\n",
    "#     @classmethod\n",
    "#     def create(cls, train_ds, valid_ds, test_ds=None, path='.', bs=32, val_bs=None, pad_idx=1,\n",
    "#                dl_tfms=None, pad_first=False, device=None, no_check=False, backwards=False, **dl_kwargs):\n",
    "#         \"Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`\"\n",
    "#         datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
    "#         val_bs = ifnone(val_bs, bs)\n",
    "#         collate_fn = partial(seq2seq_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n",
    "#         train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs//2)\n",
    "#         train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n",
    "#         dataloaders = [train_dl]\n",
    "#         for ds in datasets[1:]:\n",
    "#             lengths = [len(t) for t in ds.x.items]\n",
    "#             sampler = SortSampler(ds.x, key=lengths.__getitem__)\n",
    "#             dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n",
    "#         return cls(*dataloaders, path=path, device=device, collate_fn=collate_fn, no_check=no_check)\n",
    "\n",
    "# class Seq2SeqTextList(TextList):\n",
    "#     _bunch = Seq2SeqDataBunch\n",
    "#     _label_cls = TextList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_tokenizer_path = Path(\"method_tokenizer.model\")  \n",
    "comment_tokenizer_path = Path(\"comment_tokenizer.model\")\n",
    "ast_tokenizer_path = Path(\"ast_tokenizer.model\")\n",
    "method_processor = spm.SentencePieceProcessor(model_file=str(method_tokenizer_path))\n",
    "comment_processor = spm.SentencePieceProcessor(model_file=str(comment_tokenizer_path))\n",
    "ast_processor = spm.SentencePieceProcessor(model_file=str(ast_tokenizer_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen_dbs(df_trn, df_val, df_tst, spm_model_path, bs=96, max_seq=128):\n",
    "#     # Merge training and validation datasets with a 'valid' column\n",
    "#     df_merged = pd.concat([df_trn.assign(valid=False), df_val.assign(valid=True)])\n",
    "\n",
    "#     # Load SentencePiece tokenizer from trained model\n",
    "#     method_tok = SentencePieceTokenizer(sp_model=spm_model_path)\n",
    "#     comment_tok = SentencePieceTokenizer(sp_model=spm_model_path)\n",
    "\n",
    "#     # Define DataBlock for training\n",
    "#     dblock = DataBlock(\n",
    "#         blocks=(TextBlock.from_df(\"code\", tok=method_tok),\n",
    "#                 TextBlock.from_df(\"comment\", tok=comment_tok)),\n",
    "#         get_x=ColReader(\"code\"),\n",
    "#         get_y=ColReader(\"comment\"),\n",
    "#         splitter=ColSplitter(\"valid\"),\n",
    "#         n_inp=1\n",
    "#     )\n",
    "\n",
    "#     # Load DataLoaders\n",
    "#     db_trn = dblock.dataloaders(df_merged, bs=bs)\n",
    "\n",
    "#     # Define DataBlock for test set\n",
    "#     dblock_test = DataBlock(\n",
    "#         blocks=(TextBlock.from_df(\"code\", tok=method_tok),\n",
    "#                 TextBlock.from_df(\"comment\", tok=comment_tok)),\n",
    "#         get_x=ColReader(\"code\"),\n",
    "#         get_y=ColReader(\"comment\"),\n",
    "#         splitter=FuncSplitter(lambda x: False),  # No validation split for test\n",
    "#         n_inp=1\n",
    "#     )\n",
    "\n",
    "#     db_tst = dblock_test.dataloaders(df_tst, bs=16)\n",
    "\n",
    "#     return db_trn, db_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_sp = spm.SentencePieceProcessor()\n",
    "method_sp.load(\"method_tokenizer.model\")\n",
    "comment_sp = spm.SentencePieceProcessor()\n",
    "comment_sp.load(\"comment_tokenizer.model\")\n",
    "ast_sp = spm.SentencePieceProcessor()\n",
    "ast_sp.load(\"ast_tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn[\"code\"] = df_trn[\"code\"].apply(lambda x: \" \".join(method_sp.encode_as_pieces(x)))\n",
    "df_trn[\"comment\"] = df_trn[\"comment\"].apply(lambda x: \" \".join(comment_sp.encode_as_pieces(x)))\n",
    "df_trn[\"ast\"] = df_trn[\"ast\"].apply(lambda x: \" \".join(ast_sp.encode_as_pieces(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=(TextBlock.from_df(\"code_tok\"), TextBlock.from_df(\"comment_tok\")),\n",
    "    get_x=ColReader(\"code_tok\"),\n",
    "    get_y=ColReader(\"comment_tok\"),\n",
    "    splitter=RandomSplitter(valid_pct=0.2),  # FIXED: Random splitting instead of 'valid' column\n",
    "    n_inp=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_trn = dblock.dataloaders(df_trn, bs=96, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_sp = spm.SentencePieceProcessor()\n",
    "method_sp.load(\"method_tokenizer.model\")\n",
    "\n",
    "comment_sp = spm.SentencePieceProcessor()\n",
    "comment_sp.load(\"comment_tokenizer.model\")\n",
    "\n",
    "ast_sp = spm.SentencePieceProcessor()\n",
    "ast_sp.load(\"ast_tokenizer.model\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "df_trn[\"code\"] = df_trn[\"code\"].apply(lambda x: method_sp.encode_as_ids(x))\n",
    "df_trn[\"comment\"] = df_trn[\"comment\"].apply(lambda x: comment_sp.encode_as_ids(x))\n",
    "df_trn[\"ast\"] = df_trn[\"ast\"].apply(lambda x: comment_sp.encode_as_ids(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "comment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ast",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "83338203-d42c-45e2-8ac4-0eb3b86e87b8",
       "rows": [
        [
         "0",
         "[43, 3, 1067, 3, 31999, 3, 570, 80, 3, 12818, 3, 4, 3, 4863, 14943, 3, 8, 3, 450, 465, 3, 8, 3, 9617, 4132, 3, 5, 3, 9, 3, 1067, 851, 10, 80, 3, 1067, 3, 4, 14943, 3, 8, 3, 1067, 3, 31999, 3, 14425, 3, 292, 3, 754, 3, 6660, 341, 3, 8, 5204, 3, 71, 3, 31999, 22, 18, 3, 8, 3, 1035, 3, 31997, 888, 3, 4, 3, 5, 3, 5, 3, 6, 44, 3, 4, 364, 22, 3, 69, 3, 31997, 899, 3, 42, 4129, 3, 5, 3, 9, 267, 3, 31999, 109, 3, 4, 3, 451, 3, 31999, 2608, 3, 8, 13, 3, 7, 21, 899, 3, 31999, 192, 3, 4, 3, 5, 3, 5, 3, 6, 3515, 3, 31999, 22440, 3, 12818, 3, 4, 851, 3, 8, 899, 3, 5, 3, 6, 16, 3, 4, 3, 450, 3, 5616, 3, 815, 3, 31999, 107, 3, 31999, 3, 2656, 3, 4, 899, 3, 5, 3, 5, 3, 9, 3, 4, 3, 4, 3, 450, 3, 5616, 3, 815, 3, 5, 7490, 3, 1844, 3, 4, 851, 3, 8, 899, 3, 5, 3, 5, 3, 31999, 3, 28161, 3, 4, 465, 3, 5, 3, 6, 3, 31998, 16, 3, 4, 3, 2914, 3, 5616, 3, 815, 3, 31999, 107, 3, 31999, 3, 2656, 3, 4, 899, 3, 5, 3, 5, 3, 9, 3, 4, 3, 4, 3, 2914, 3, 5616, 3, 815, 3, 5, 7490, 3, 1844, 3, 4, 851, 3, 8, 899, 3, 5, 3, 5, 3, 31999, 59, 3, 9617, 3, 4, 4132, 3, 5, 3, 6, 3, 31998, 3, 31998, 851, 3, 31999, 35, 3, 6711, 3, 4435, 3, 4, 3, 5, 3, 31999, 151, 3, 4, 3, 5, 3, 6, 14, 851, 3, 31999, 9356, 3, 27, 3, 4, 3, 5, 3, 6, 3, 31998]",
         "[32, 705, 844, 493, 20, 4, 2137, 4, 548, 24, 89, 33, 3552, 12, 717, 667, 7, 7538, 3]",
         "[46, 0, 46, 0, 209, 46, 15190, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 46, 14656, 46, 0, 5, 7505, 2168, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 5, 7505, 2168, 46, 0, 3331, 807, 46, 12688, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 323, 6171, 46, 0, 46, 23455, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 323, 6171, 46, 0, 46, 23455, 46, 0, 46, 0, 323, 6171, 46, 0, 46, 23455, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 323, 6171, 46, 0, 46, 23455, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 5, 7505, 2168, 46, 0, 3331, 807, 46, 12688, 46, 0, 46, 0, 46, 14656, 46, 0, 5, 7505, 2168, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 36, 46, 0, 46, 17028, 46, 0, 46, 0, 261, 1371, 13, 2403, 318, 46, 0, 36, 46, 0, 864, 46, 18083, 946, 46, 0, 46, 0, 5, 7505, 2168, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 323, 6171, 46, 0, 46, 23455, 46, 0, 46, 0, 323, 6171, 46, 0, 46, 23455, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 5, 7505, 2168, 46, 0, 3331, 807, 46, 12688, 46, 0, 46, 0, 5, 7505, 2168, 46, 0, 3331, 807, 46, 12688, 46, 0, 46, 0, 5, 7505, 2168, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 261, 1371, 13, 2403, 318, 46, 0, 36, 46, 0, 864, 46, 18083, 946, 46, 0, 46, 0, 594, 46, 0, 46, 17028, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 14, 3330, 46, 0, 700, 5, 3311, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 14, 3330, 46, 0, 700, 5, 3311, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 504, 46, 0, 46, 17028, 46, 0, 46, 0, 3260, 61, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 3260, 61, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 594, 46, 0, 46, 17028, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 867, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 867, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 594, 46, 0, 46, 17028, 46, 0, 46, 0, 504, 46, 0, 46, 17028, 46, 0, 46, 0, 504, 46, 0, 46, 17028, 46, 0, 46, 0, 3260, 61, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 3260, 61, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 594, 46, 0, 46, 17028, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 867, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 867, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 594, 46, 0, 46, 17028, 46, 0, 46, 0, 504, 46, 0, 46, 17028, 46, 0, 46, 0, 594, 46, 0, 46, 17028, 46, 0, 46, 0, 36, 46, 0, 46, 17028, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 209, 1180, 46, 0, 46, 17028, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 1180, 46, 0, 46, 17028, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 209, 3747, 5, 8826]"
        ],
        [
         "1",
         "[43, 34, 1090, 3, 638, 3, 4, 19, 62, 3, 8, 19, 102, 3, 5, 3, 9, 16, 3, 4, 3, 4, 102, 22, 15, 3, 7, 3, 5, 111, 111, 3, 4, 102, 3, 31997, 10, 317, 3, 5, 111, 111, 3, 4, 62, 22, 15, 3, 7, 3, 5, 111, 111, 3, 4, 62, 3, 31997, 10, 301, 3, 5, 3, 5, 3, 9, 14, 30, 3, 7, 3, 6, 3, 31998, 14, 30, 3, 7, 3, 6, 3, 31998]",
         "[412, 5, 387, 77, 6, 353, 19, 4, 161, 323, 472, 527, 46, 179, 3256, 54, 4, 1651, 40, 3]",
         "[46, 0, 46, 0, 209, 46, 15190, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 14920, 46, 0, 323, 6171, 46, 0, 46, 0, 14920, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 14920, 46, 0, 323, 6171, 46, 0, 46, 0, 14920, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 14920, 46, 0, 323, 6171, 46, 0, 46, 0, 14920, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 504, 46, 0, 46, 17028, 46, 0, 46, 0, 14, 3330, 46, 0, 700, 5, 3311, 46, 0, 46, 0, 14, 3330, 46, 0, 700, 5, 3311, 46, 0, 46, 0, 14, 3330, 46, 0, 700, 5, 3311, 46, 0, 46, 0, 14, 3330, 46, 0, 700, 5, 3311, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 14, 3330, 46, 0, 700, 5, 3311, 46, 0, 46, 0, 14, 3330, 46, 0, 700, 5, 3311, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 14, 3330, 46, 0, 700, 5, 3311, 46, 0, 46, 0, 14, 3330, 46, 0, 700, 5, 3311, 46, 0, 46, 0, 14, 3330, 46, 0, 700, 5, 3311, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 14, 3330, 46, 0, 700, 5, 3311, 46, 0, 46, 0, 14, 3330, 46, 0, 700, 5, 3311, 46, 0, 46, 0, 14, 3330, 46, 0, 700, 5, 3311, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 14, 3330, 46, 0, 700, 5, 3311, 46, 0, 46, 0, 14, 3330, 46, 0, 700, 5, 3311, 46, 0, 46, 0, 594, 46, 0, 46, 17028, 46, 0, 46, 0, 209, 1180, 46, 0, 46, 17028, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 209, 1180, 46, 0, 46, 17028, 46, 0, 46, 0, 594, 46, 0, 46, 17028, 46, 0, 46, 0, 504, 46, 0, 46, 17028, 46, 0, 46, 0, 209, 1180, 46, 0, 46, 17028, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 209, 1180, 46, 0, 46, 17028, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 209, 3747, 5, 8826]"
        ],
        [
         "2",
         "[43, 3, 16118, 2303, 3, 356, 3, 3543, 3, 4, 3, 105, 104, 3, 8, 3, 153, 152, 3, 8, 18, 2956, 3, 8, 18, 5664, 3, 8, 139, 22, 3, 2354, 3, 373, 3, 31997, 2303, 3, 1487, 3, 5, 36, 3, 356, 3, 29, 3, 9, 150, 3, 1669, 3, 4, 152, 3, 31999, 1717, 3, 156, 3, 4, 3, 5, 3, 5, 3, 6, 1900, 3, 1042, 3, 1046, 3, 4, 152, 3, 31999, 1717, 3, 156, 3, 4, 3, 5, 3, 5, 3, 6, 3, 16118, 540, 10, 168, 3, 356, 3, 4, 104, 3, 8, 5725, 3, 81, 3, 4, 2956, 3, 5, 3, 5, 3, 6, 14, 2303, 3, 356, 3, 4, 104, 3, 8, 152, 3, 8, 540, 3, 8, 5664, 3, 8, 2303, 3, 1487, 3, 5, 3, 6, 3, 31998]",
         "[1163, 209, 61, 4, 18, 247, 10, 78, 912, 145, 690, 296, 19, 37, 11, 13, 704, 134, 135, 1634, 206, 3]",
         "[46, 0, 46, 0, 209, 46, 15190, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 323, 6171, 46, 0, 46, 23455, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 323, 6171, 46, 0, 46, 23455, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 46, 14656, 46, 0, 5, 7505, 2168, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 5, 7505, 2168, 46, 0, 3331, 807, 46, 12688, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 5, 7505, 2168, 46, 0, 3331, 807, 46, 12688, 46, 0, 46, 0, 46, 14656, 46, 0, 5, 7505, 2168, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 209, 1180, 46, 0, 46, 17028, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 1180, 46, 0, 46, 17028, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 209, 3747, 5, 8826]"
        ],
        [
         "3",
         "[43, 23, 18133, 3, 4, 3, 5, 3, 9, 2372, 3, 155, 3, 31999, 18133, 3, 4, 3, 5, 3, 6, 3172, 3, 5352, 3, 31999, 6257, 3, 3113, 3, 289, 3, 334, 3, 4, 2372, 3, 155, 3, 31999, 1109, 3, 3113, 3, 289, 3, 4, 3, 5, 3, 5, 3, 6, 3172, 3, 5352, 3, 31999, 6257, 3, 25901, 3, 334, 3, 4, 2372, 3, 155, 3, 31999, 1109, 3, 25901, 3, 4, 3, 5, 3, 5, 3, 6, 3, 31998]",
         "[6, 1164, 193, 3]",
         "[46, 0, 46, 0, 209, 46, 15190, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 209, 3747, 5, 8826]"
        ],
        [
         "4",
         "[12, 3, 11094, 3, 495, 3, 10864, 3, 4, 13933, 11068, 3, 5, 3, 9, 57, 3, 4, 20, 3, 8, 20, 3, 8, 11068, 3, 8, 20, 3, 5, 3, 6, 26, 3, 31999, 9528, 3, 27, 10, 17, 193, 3, 31999, 706, 3, 31999, 195, 22, 3, 10864, 3, 31997, 3, 4, 3, 5, 3, 6, 3, 31998]",
         "[21, 5, 17, 1461, 3]",
         "[46, 0, 46, 0, 35, 2746, 878, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 130, 6736, 46, 0, 35, 2746, 878, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 10542, 2357, 46, 0, 46, 0, 10542, 2357, 46, 0, 46, 0, 10542, 2357, 46, 0, 46, 0, 10542, 2357, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 10542, 2357, 46, 0, 46, 0, 10542, 2357, 46, 0, 46, 0, 130, 6736, 46, 0, 35, 2746, 878, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 61, 1139, 46, 3135, 46, 0, 46, 0, 3136, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 46, 13724, 46, 0, 209, 46, 10517, 46, 0, 46, 0, 3136, 46, 0, 46, 0, 3260, 61, 46, 0, 466, 5, 4768, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 323, 6171, 46, 0, 46, 23455, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 323, 6171, 46, 0, 46, 23455, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 3260, 61, 46, 0, 466, 5, 4768, 46, 0, 46, 0, 61, 1139, 46, 3135, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 35, 2746, 878, 46, 0, 209, 3747, 5, 8826]"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>comment</th>\n",
       "      <th>ast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[43, 3, 1067, 3, 31999, 3, 570, 80, 3, 12818, 3, 4, 3, 4863, 14943, 3, 8, 3, 450, 465, 3, 8, 3, 9617, 4132, 3, 5, 3, 9, 3, 1067, 851, 10, 80, 3, 1067, 3, 4, 14943, 3, 8, 3, 1067, 3, 31999, 3, 14425, 3, 292, 3, 754, 3, 6660, 341, 3, 8, 5204, 3, 71, 3, 31999, 22, 18, 3, 8, 3, 1035, 3, 31997, 888, 3, 4, 3, 5, 3, 5, 3, 6, 44, 3, 4, 364, 22, 3, 69, 3, 31997, 899, 3, 42, 4129, 3, 5, 3, 9, 267, 3, 31999, 109, 3, ...]</td>\n",
       "      <td>[32, 705, 844, 493, 20, 4, 2137, 4, 548, 24, 89, 33, 3552, 12, 717, 667, 7, 7538, 3]</td>\n",
       "      <td>[46, 0, 46, 0, 209, 46, 15190, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[43, 34, 1090, 3, 638, 3, 4, 19, 62, 3, 8, 19, 102, 3, 5, 3, 9, 16, 3, 4, 3, 4, 102, 22, 15, 3, 7, 3, 5, 111, 111, 3, 4, 102, 3, 31997, 10, 317, 3, 5, 111, 111, 3, 4, 62, 22, 15, 3, 7, 3, 5, 111, 111, 3, 4, 62, 3, 31997, 10, 301, 3, 5, 3, 5, 3, 9, 14, 30, 3, 7, 3, 6, 3, 31998, 14, 30, 3, 7, 3, 6, 3, 31998]</td>\n",
       "      <td>[412, 5, 387, 77, 6, 353, 19, 4, 161, 323, 472, 527, 46, 179, 3256, 54, 4, 1651, 40, 3]</td>\n",
       "      <td>[46, 0, 46, 0, 209, 46, 15190, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 14920, 46, 0, 323, 6171, 46, 0, 46, 0, 14920, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 14920, 46, 0, 323, 6171, 46, 0, 46, 0, 14920, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 14920, 46, 0, 323, 6171, 46, 0, 46, 0, 14920, 46, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[43, 3, 16118, 2303, 3, 356, 3, 3543, 3, 4, 3, 105, 104, 3, 8, 3, 153, 152, 3, 8, 18, 2956, 3, 8, 18, 5664, 3, 8, 139, 22, 3, 2354, 3, 373, 3, 31997, 2303, 3, 1487, 3, 5, 36, 3, 356, 3, 29, 3, 9, 150, 3, 1669, 3, 4, 152, 3, 31999, 1717, 3, 156, 3, 4, 3, 5, 3, 5, 3, 6, 1900, 3, 1042, 3, 1046, 3, 4, 152, 3, 31999, 1717, 3, 156, 3, 4, 3, 5, 3, 5, 3, 6, 3, 16118, 540, 10, 168, 3, 356, 3, 4, 104, 3, 8, ...]</td>\n",
       "      <td>[1163, 209, 61, 4, 18, 247, 10, 78, 912, 145, 690, 296, 19, 37, 11, 13, 704, 134, 135, 1634, 206, 3]</td>\n",
       "      <td>[46, 0, 46, 0, 209, 46, 15190, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 209, 46, 10517, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[43, 23, 18133, 3, 4, 3, 5, 3, 9, 2372, 3, 155, 3, 31999, 18133, 3, 4, 3, 5, 3, 6, 3172, 3, 5352, 3, 31999, 6257, 3, 3113, 3, 289, 3, 334, 3, 4, 2372, 3, 155, 3, 31999, 1109, 3, 3113, 3, 289, 3, 4, 3, 5, 3, 5, 3, 6, 3172, 3, 5352, 3, 31999, 6257, 3, 25901, 3, 334, 3, 4, 2372, 3, 155, 3, 31999, 1109, 3, 25901, 3, 4, 3, 5, 3, 5, 3, 6, 3, 31998]</td>\n",
       "      <td>[6, 1164, 193, 3]</td>\n",
       "      <td>[46, 0, 46, 0, 209, 46, 15190, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[12, 3, 11094, 3, 495, 3, 10864, 3, 4, 13933, 11068, 3, 5, 3, 9, 57, 3, 4, 20, 3, 8, 20, 3, 8, 11068, 3, 8, 20, 3, 5, 3, 6, 26, 3, 31999, 9528, 3, 27, 10, 17, 193, 3, 31999, 706, 3, 31999, 195, 22, 3, 10864, 3, 31997, 3, 4, 3, 5, 3, 6, 3, 31998]</td>\n",
       "      <td>[21, 5, 17, 1461, 3]</td>\n",
       "      <td>[46, 0, 46, 0, 35, 2746, 878, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 130, 6736, 46, 0, 35, 2746, 878, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 10542, 2357, 46, 0, 46, 0, 10542, 2357, 46, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                            code  \\\n",
       "0  [43, 3, 1067, 3, 31999, 3, 570, 80, 3, 12818, 3, 4, 3, 4863, 14943, 3, 8, 3, 450, 465, 3, 8, 3, 9617, 4132, 3, 5, 3, 9, 3, 1067, 851, 10, 80, 3, 1067, 3, 4, 14943, 3, 8, 3, 1067, 3, 31999, 3, 14425, 3, 292, 3, 754, 3, 6660, 341, 3, 8, 5204, 3, 71, 3, 31999, 22, 18, 3, 8, 3, 1035, 3, 31997, 888, 3, 4, 3, 5, 3, 5, 3, 6, 44, 3, 4, 364, 22, 3, 69, 3, 31997, 899, 3, 42, 4129, 3, 5, 3, 9, 267, 3, 31999, 109, 3, ...]   \n",
       "1                                                                                                            [43, 34, 1090, 3, 638, 3, 4, 19, 62, 3, 8, 19, 102, 3, 5, 3, 9, 16, 3, 4, 3, 4, 102, 22, 15, 3, 7, 3, 5, 111, 111, 3, 4, 102, 3, 31997, 10, 317, 3, 5, 111, 111, 3, 4, 62, 22, 15, 3, 7, 3, 5, 111, 111, 3, 4, 62, 3, 31997, 10, 301, 3, 5, 3, 5, 3, 9, 14, 30, 3, 7, 3, 6, 3, 31998, 14, 30, 3, 7, 3, 6, 3, 31998]   \n",
       "2          [43, 3, 16118, 2303, 3, 356, 3, 3543, 3, 4, 3, 105, 104, 3, 8, 3, 153, 152, 3, 8, 18, 2956, 3, 8, 18, 5664, 3, 8, 139, 22, 3, 2354, 3, 373, 3, 31997, 2303, 3, 1487, 3, 5, 36, 3, 356, 3, 29, 3, 9, 150, 3, 1669, 3, 4, 152, 3, 31999, 1717, 3, 156, 3, 4, 3, 5, 3, 5, 3, 6, 1900, 3, 1042, 3, 1046, 3, 4, 152, 3, 31999, 1717, 3, 156, 3, 4, 3, 5, 3, 5, 3, 6, 3, 16118, 540, 10, 168, 3, 356, 3, 4, 104, 3, 8, ...]   \n",
       "3                                                                       [43, 23, 18133, 3, 4, 3, 5, 3, 9, 2372, 3, 155, 3, 31999, 18133, 3, 4, 3, 5, 3, 6, 3172, 3, 5352, 3, 31999, 6257, 3, 3113, 3, 289, 3, 334, 3, 4, 2372, 3, 155, 3, 31999, 1109, 3, 3113, 3, 289, 3, 4, 3, 5, 3, 5, 3, 6, 3172, 3, 5352, 3, 31999, 6257, 3, 25901, 3, 334, 3, 4, 2372, 3, 155, 3, 31999, 1109, 3, 25901, 3, 4, 3, 5, 3, 5, 3, 6, 3, 31998]   \n",
       "4                                                                                                                                                                          [12, 3, 11094, 3, 495, 3, 10864, 3, 4, 13933, 11068, 3, 5, 3, 9, 57, 3, 4, 20, 3, 8, 20, 3, 8, 11068, 3, 8, 20, 3, 5, 3, 6, 26, 3, 31999, 9528, 3, 27, 10, 17, 193, 3, 31999, 706, 3, 31999, 195, 22, 3, 10864, 3, 31997, 3, 4, 3, 5, 3, 6, 3, 31998]   \n",
       "\n",
       "                                                                                                comment  \\\n",
       "0                  [32, 705, 844, 493, 20, 4, 2137, 4, 548, 24, 89, 33, 3552, 12, 717, 667, 7, 7538, 3]   \n",
       "1               [412, 5, 387, 77, 6, 353, 19, 4, 161, 323, 472, 527, 46, 179, 3256, 54, 4, 1651, 40, 3]   \n",
       "2  [1163, 209, 61, 4, 18, 247, 10, 78, 912, 145, 690, 296, 19, 37, 11, 13, 704, 134, 135, 1634, 206, 3]   \n",
       "3                                                                                     [6, 1164, 193, 3]   \n",
       "4                                                                                  [21, 5, 17, 1461, 3]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ast  \n",
       "0                    [46, 0, 46, 0, 209, 46, 15190, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, ...]  \n",
       "1                         [46, 0, 46, 0, 209, 46, 15190, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 14920, 46, 0, 323, 6171, 46, 0, 46, 0, 14920, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 14920, 46, 0, 323, 6171, 46, 0, 46, 0, 14920, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 14920, 46, 0, 323, 6171, 46, 0, 46, 0, 14920, 46, ...]  \n",
       "2                       [46, 0, 46, 0, 209, 46, 15190, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 209, 46, 10517, ...]  \n",
       "3  [46, 0, 46, 0, 209, 46, 15190, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 209, 46, 15190, 46, 0, 261, 1741, 443, 1524, 46, 0, ...]  \n",
       "4                [46, 0, 46, 0, 35, 2746, 878, 46, 0, 209, 3747, 5, 8826, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 209, 46, 10517, 46, 0, 323, 6171, 46, 0, 46, 0, 36, 16444, 46, 0, 5, 421, 5, 6471, 46, 0, 46, 0, 46, 17028, 46, 0, 6034, 46, 14087, 46, 0, 46, 0, 130, 6736, 46, 0, 35, 2746, 878, 46, 0, 261, 1741, 443, 1524, 46, 0, 46, 0, 10542, 2357, 46, 0, 46, 0, 10542, 2357, 46, ...]  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeCommentDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.codes = df[\"code\"].tolist()\n",
    "        self.comments = df[\"comment\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.codes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.codes[idx]), torch.tensor(self.comments[idx])\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = CodeCommentDataset(df_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  For padding purposes\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)  # Unzip source and target sequences\n",
    "    src_batch = pad_sequence([torch.tensor(x) for x in src_batch], batch_first=True, padding_value=1)  # PAD_IDX=1\n",
    "    tgt_batch = pad_sequence([torch.tensor(x) for x in tgt_batch], batch_first=True, padding_value=1)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CodeCommentDataset at 0x1accc941e10>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Encode the position with a sinusoid.\"\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.register_buffer('freq', 1 / (10000 ** (torch.arange(0., d, 2.)/d)))\n",
    "    \n",
    "    def forward(self, pos):\n",
    "        inp = torch.outer(pos, self.freq)\n",
    "        enc = torch.cat([inp.sin(), inp.cos()], dim=-1)\n",
    "        return enc\n",
    "\n",
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_sz, emb_sz, inp_p=0.):\n",
    "        super().__init__()\n",
    "        self.emb_sz = emb_sz\n",
    "        self.embed = nn.Embedding(vocab_sz, emb_sz)  # Fix embedding call\n",
    "        self.pos_enc = PositionalEncoding(emb_sz)\n",
    "        self.drop = nn.Dropout(inp_p)\n",
    "    \n",
    "    def forward(self, inp): \n",
    "        pos = torch.arange(0, inp.size(1), device=inp.device).float()\n",
    "        return self.drop(self.embed(inp) * math.sqrt(self.emb_sz) + self.pos_enc(pos))\n",
    "\n",
    "def feed_forward(d_model, d_ff, ff_p=0., double_drop=True):\n",
    "    layers = [nn.Linear(d_model, d_ff), nn.ReLU()]\n",
    "    if double_drop:\n",
    "        layers.append(nn.Dropout(ff_p))\n",
    "    \n",
    "    return nn.Sequential(\n",
    "        *layers,\n",
    "        nn.Linear(d_ff, d_model),\n",
    "        nn.Dropout(ff_p),\n",
    "        nn.LayerNorm(d_model)  # Removed `MergeLayer()`\n",
    "    )\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_head=None, p=0., bias=True, scale=True):\n",
    "        super().__init__()\n",
    "        d_head = d_head if d_head is not None else d_model // n_heads  # Fix d_head\n",
    "        \n",
    "        self.n_heads, self.d_head, self.scale = n_heads, d_head, scale\n",
    "        self.q_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.k_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.v_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        \n",
    "        self.out = nn.Linear(n_heads * d_head, d_model, bias=bias)\n",
    "        self.drop_att, self.drop_res = nn.Dropout(p), nn.Dropout(p)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, q, kv, mask=None):\n",
    "        return self.ln(q + self.drop_res(self.out(self._apply_attention(q, kv, mask=mask))))\n",
    "    \n",
    "    def create_attn_mat(self, x, layer, bs):\n",
    "        return layer(x).view(bs, x.size(1), self.n_heads, self.d_head).permute(0, 2, 1, 3)\n",
    "    \n",
    "    def _apply_attention(self, q, kv, mask=None):\n",
    "        bs, seq_len = q.size(0), q.size(1)\n",
    "        wq, wk, wv = map(lambda o: self.create_attn_mat(*o, bs), zip((q, kv, kv), (self.q_wgt, self.k_wgt, self.v_wgt)))\n",
    "        \n",
    "        attn_score = wq @ wk.transpose(2, 3)\n",
    "        if self.scale:\n",
    "            attn_score /= math.sqrt(self.d_head)\n",
    "        if mask is not None:\n",
    "            attn_score = attn_score.masked_fill(mask.bool(), float('-inf'))  # Fix mask handling\n",
    "        \n",
    "        attn_prob = self.drop_att(nn.functional.softmax(attn_score, dim=-1))\n",
    "        attn_vec = attn_prob @ wv\n",
    "        return attn_vec.permute(0, 2, 1, 3).contiguous().view(bs, seq_len, -1)\n",
    "\n",
    "\n",
    "def get_output_mask(inp, pad_idx=1):\n",
    "    return torch.triu(inp.new_ones(inp.size(1), inp.size(1)), diagonal=1)[None, None].bool()\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"Encoder block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, n_heads, d_model, d_head, d_inner, p=0., bias=True, scale=True, double_drop=True):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(n_heads, d_model, d_head, p=p, bias=bias, scale=scale)\n",
    "        self.ff  = feed_forward(d_model, d_inner, ff_p=p, double_drop=double_drop)\n",
    "    \n",
    "    def forward(self, x, mask=None): return self.ff(self.mha(x, x, mask=mask))\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"Decoder block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, n_heads, d_model, d_head, d_inner, p=0., bias=True, scale=True, double_drop=True):\n",
    "        super().__init__()\n",
    "        self.mha1 = MultiHeadAttention(n_heads, d_model, d_head, p=p, bias=bias, scale=scale)\n",
    "        self.mha2 = MultiHeadAttention(n_heads, d_model, d_head, p=p, bias=bias, scale=scale)\n",
    "        self.ff   = feed_forward(d_model, d_inner, ff_p=p, double_drop=double_drop)\n",
    "    \n",
    "    def forward(self, x, enc, mask_out=None): return self.ff(self.mha2(self.mha1(x, x, mask_out), enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, inp_vsz, out_vsz, n_layers=6, n_heads=8, d_model=256, d_head=32, \n",
    "                 d_inner=1024, p=0.1, bias=True, scale=True, double_drop=True, pad_idx=1):\n",
    "        super().__init__()  # Initialize nn.Module properly\n",
    "        self.enc_emb = TransformerEmbedding(inp_vsz, d_model, p)\n",
    "        self.dec_emb = TransformerEmbedding(out_vsz, d_model, 0.0)\n",
    "        args = (n_heads, d_model, d_head, d_inner, p, bias, scale, double_drop)\n",
    "        self.encoder = nn.ModuleList([EncoderBlock(*args) for _ in range(n_layers)])\n",
    "        self.decoder = nn.ModuleList([DecoderBlock(*args) for _ in range(n_layers)])\n",
    "        self.out = nn.Linear(d_model, out_vsz)\n",
    "        self.out.weight = self.dec_emb.embed.weight  # Weight tying\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def forward(self, inp, out):\n",
    "        mask_out = get_output_mask(out, self.pad_idx)  # Ensure this function exists\n",
    "        enc, out = self.enc_emb(inp), self.dec_emb(out)\n",
    "        for layer in self.encoder:\n",
    "            enc = layer(enc)  # Apply each encoder layer sequentiall\n",
    "        for layer in self.decoder:\n",
    "            out = layer(out, enc, mask_out)  # Decoder processes input & encoded output\n",
    "        return self.out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callback.core import Callback\n",
    "# For Evaluation Purposes\n",
    "class NGram():\n",
    "    def __init__(self, ngram, max_n=5000): self.ngram,self.max_n = ngram,max_n\n",
    "    def __eq__(self, other):\n",
    "        if len(self.ngram) != len(other.ngram): return False\n",
    "        return np.all(np.array(self.ngram) == np.array(other.ngram))\n",
    "    def __hash__(self): return int(sum([o * self.max_n**i for i,o in enumerate(self.ngram)]))\n",
    "\n",
    "def get_grams(x, n, max_n=5000):\n",
    "    return x if n==1 else [NGram(x[i:i+n], max_n=max_n) for i in range(len(x)-n+1)]\n",
    "\n",
    "def get_correct_ngrams(pred, targ, n, max_n=5000):\n",
    "    pred_grams,targ_grams = get_grams(pred, n, max_n=max_n),get_grams(targ, n, max_n=max_n)\n",
    "    pred_cnt,targ_cnt = Counter(pred_grams),Counter(targ_grams)\n",
    "    return sum([min(c, targ_cnt[g]) for g,c in pred_cnt.items()]),len(pred_grams)\n",
    "\n",
    "class CorpusBLEU(Callback):\n",
    "    def __init__(self, vocab_sz):\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.name = 'bleu'\n",
    "    \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.pred_len,self.targ_len,self.corrects,self.counts = 0,0,[0]*4,[0]*4\n",
    "    \n",
    "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "        last_output = last_output.argmax(dim=-1)\n",
    "        for pred,targ in zip(last_output.cpu().numpy(),last_target.cpu().numpy()):\n",
    "            self.pred_len += len(pred)\n",
    "            self.targ_len += len(targ)\n",
    "            for i in range(4):\n",
    "                c,t = get_correct_ngrams(pred, targ, i+1, max_n=self.vocab_sz)\n",
    "                self.corrects[i] += c\n",
    "                self.counts[i]   += t\n",
    "    \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        precs = [c/t for c,t in zip(self.corrects,self.counts)]\n",
    "        len_penalty = exp(1 - self.targ_len/self.pred_len) if self.pred_len < self.targ_len else 1\n",
    "        bleu = len_penalty * ((precs[0]*precs[1]*precs[2]*precs[3]) ** 0.25)\n",
    "        return add_metrics(last_metrics, bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_vocab_size = len(df_trn[\"code\"].tolist())  # Adjust based on preprocessing\n",
    "out_vocab_size = len(df_trn[\"comment\"].tolist())\n",
    "\n",
    "model = Transformer(inp_vocab_size, out_vocab_size, d_model=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src, tgt = zip(*batch)  # Unzip the batch into source and target sequences\n",
    "\n",
    "    # Convert to tensors\n",
    "    src = [torch.tensor(seq, dtype=torch.long) for seq in src]\n",
    "    tgt = [torch.tensor(seq, dtype=torch.long) for seq in tgt]\n",
    "\n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    src_padded = pad_sequence(src, batch_first=True, padding_value=0)\n",
    "    tgt_padded = pad_sequence(tgt, batch_first=True, padding_value=0)\n",
    "\n",
    "    return src_padded, tgt_padded\n",
    "\n",
    "# Update DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,  # Your dataset object\n",
    "    batch_size=32,  # Adjust batch size as needed\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn  # Use custom collate function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 32000\n"
     ]
    }
   ],
   "source": [
    "with open(\"tokenizer.vocab\", \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab = [line.strip() for line in f.readlines()]\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print(f\"Vocabulary Size: {VOCAB_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 32000\n",
    "LR = 3e-4\n",
    "EPOCHS = 8\n",
    "model_name = \"comment_gen\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (enc_emb): TransformerEmbedding(\n",
      "    (embed): Embedding(445812, 256)\n",
      "    (pos_enc): PositionalEncoding()\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (dec_emb): TransformerEmbedding(\n",
      "    (embed): Embedding(445812, 256)\n",
      "    (pos_enc): PositionalEncoding()\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): ModuleList(\n",
      "    (0-5): 6 x EncoderBlock(\n",
      "      (mha): MultiHeadAttention(\n",
      "        (q_wgt): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (k_wgt): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (v_wgt): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (drop_att): Dropout(p=0.1, inplace=False)\n",
      "        (drop_res): Dropout(p=0.1, inplace=False)\n",
      "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (ff): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (4): Dropout(p=0.1, inplace=False)\n",
      "        (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): ModuleList(\n",
      "    (0-5): 6 x DecoderBlock(\n",
      "      (mha1): MultiHeadAttention(\n",
      "        (q_wgt): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (k_wgt): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (v_wgt): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (drop_att): Dropout(p=0.1, inplace=False)\n",
      "        (drop_res): Dropout(p=0.1, inplace=False)\n",
      "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (mha2): MultiHeadAttention(\n",
      "        (q_wgt): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (k_wgt): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (v_wgt): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (drop_att): Dropout(p=0.1, inplace=False)\n",
      "        (drop_res): Dropout(p=0.1, inplace=False)\n",
      "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (ff): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (4): Dropout(p=0.1, inplace=False)\n",
      "        (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=256, out_features=445812, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(learn, epochs, model_name, max_lr = 5e-4):\n",
    "    \"\"\"Trains a model using save model, early stopping, and show graph call backs.\"\"\"\n",
    "    callback_fns = [\n",
    "        callbacks.SaveModelCallback(\n",
    "            learn, every='improvement',\n",
    "            monitor='valid_loss', name=f'{model_name}_save_model'\n",
    "        ),\n",
    "        callbacks.EarlyStoppingCallback(\n",
    "            learn, monitor='valid_loss', min_delta = 0.01,\n",
    "            patience = 3\n",
    "        ),\n",
    "        ShowGraph(learn)\n",
    "    ]\n",
    "\n",
    "    learn.fit_one_cycle(epochs, max_lr, div_factor=5, callbacks = callback_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   epoch  train_loss  valid_loss  accuracy      bleu   time\n",
      "0      0    1.182219    1.133453  0.828182  0.791774  25:46\n",
      "1      1    0.920205    0.954264  0.841556  0.799681  25:47\n",
      "2      2    0.812330    0.875513  0.849487  0.804000  23:44\n",
      "3      3    0.752023    0.828835  0.853668  0.807183  23:45\n",
      "4      4    0.679716    0.794862  0.856593  0.809325  24:43\n",
      "5      5    0.653454    0.777795  0.859418  0.811010  25:42\n",
      "6      6    0.611860    0.770059  0.860419  0.812164  25:49\n",
      "7      7    0.605370    0.769881  0.860601  0.812119  24:45\n",
      "\n",
      "Better model found at epoch 0 with valid_loss value: 1.133453130722046.\n",
      "Better model found at epoch 1 with valid_loss value: 0.9542644023895264.\n",
      "Better model found at epoch 2 with valid_loss value: 0.8755126595497131.\n",
      "Better model found at epoch 3 with valid_loss value: 0.8288350701332092.\n",
      "Better model found at epoch 4 with valid_loss value: 0.7948615550994873.\n",
      "Better model found at epoch 5 with valid_loss value: 0.7777946591377258.\n",
      "Better model found at epoch 6 with valid_loss value: 0.7700592279434204.\n",
      "Better model found at epoch 7 with valid_loss value: 0.7698812484741211.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASIAAAEiCAYAAABdvt+2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAvoklEQVR4nO3deXiMZ9vH8e9MIpE9QRCksTQi0kitrb32rWotbWNreXRBN9pqLaXL0xYtRctLW7qqagW1FFVVtZQWoaR47C1KixC7zP3+ccoQaxIzuWc5P8cxR2bumcyccSQ/93Jd52UxDMNAKaVMZDW7AKWU0iBSSplOg0gpZToNIqWU6TSIlFKm0yBSSplOg0gpZToNIqWU6XzNLgDAZrOxf/9+QkJCsFgsZpejlHIAwzA4ceIEJUqUwGq98T6PSwTR/v37iY6ONrsMpZQT7Nu3j1KlSt3wNS4RRCEhIYAUHBoaanI1SilHOH78ONHR0fa/7xtxiSDKOhwLDQ3VIFLKw+TkdIuerFZKmU6DSCllOg0ipZTpXOIckXIum83GuXPnzC5DeZgCBQrg4+PjkPfSIPJw586dY9euXdhsNrNLUR4oPDyc4sWL3/L4Pw0iD2YYBgcOHMDHx4fo6OibDipTKqcMw+DUqVMcOnQIgKioqFt6P7cMIpsNzp2DggXNrsS1XbhwgVOnTlGiRAkCAwPNLkd5mICAAAAOHTpE0aJFb+kwze3+ixw/HkqWhHffNbsS15eZmQmAn5+fyZUoT5X1H9z58+dv6X3cLoh8fODgQZg1y+xK3IfO31PO4qjfLbcLojZt5Ovq1bB/v7m1KKUcw+2CqEQJuOsuuT9njrm1KPdRunRpxowZY3YZ6jrcLogA2rWTrykp5tahHM9isdzwNmzYsDy979q1a+ndu/ct1XbPPffw9NNP39J7qGtzy6tmbdvCwIHwww9w7BiEh5tckHKYAwcO2O9Pnz6doUOHsnXrVvu24OBg+33DMMjMzMTX9+a/xpGRkY4tVDmUW+4RxcVBfDxcuADz55tdjXKk4sWL229hYWFYLBb74z/++IOQkBAWLFhA1apV8ff35+eff2bHjh20adOGYsWKERwcTPXq1fn++++zve+Vh2YWi4UPPviAdu3aERgYSGxsLHNu8Vj/m2++ISEhAX9/f0qXLs3bb7+d7fn333+f2NhYChYsSLFixejYsaP9ua+//prExEQCAgIoXLgwjRs35uTJk7dUjztxyyAC2SsCvXqWG4YBJ0+ac3PkwuYDBw7kzTffJC0tjUqVKpGRkUHLli1ZsmQJ69evp3nz5rRu3Zq9e/fe8H2GDx9Op06d2LhxIy1btiQ5OZkjR47kqabffvuNTp068cADD7Bp0yaGDRvGkCFDmDp1KgC//vorTz75JK+88gpbt27lu+++o169eoDsBT744IM88sgjpKWl8eOPP9K+fXu8ajV4wwWkp6cbgJGenp7j71mzxjDAMIKDDeP0aScW58ZOnz5tbNmyxTh98R8oI0P+zcy4ZWTkvv4pU6YYYWFh9sdLly41AGPWrFk3/d6EhARj3Lhx9scxMTHG6NGj7Y8BY/DgwfbHGRkZBmAsWLDguu9Zv35946mnnrrmcw899JDRpEmTbNuee+45o2LFioZhGMY333xjhIaGGsePH7/qe3/77TcDMHbv3n3Tn8vVXPk7drnc/F277R5R1aoysDEjA5YsMbsalZ+qVauW7XFGRgYDBgwgPj6e8PBwgoODSUtLu+keUaVKlez3g4KCCA0NtU9ZyK20tDRq166dbVvt2rXZvn07mZmZNGnShJiYGMqWLUvXrl35/PPPOXXqFABJSUk0atSIxMRE7r//fiZPnszRo0fzVIe7ctsgslovHZ7p1bOcCQyU4Dbj5sgZJkFBQdkeDxgwgJSUFP773/+yfPlyNmzYQGJi4k07DhQoUCDbY4vF4rTJwSEhIaxbt45p06YRFRXF0KFDSUpK4tixY/j4+LB48WIWLFhAxYoVGTduHHFxcezatcsptbgitw0iuBREc+bAxdkM6gYsFggKMufmzMHdK1asoEePHrRr147ExESKFy/O7t27nfeB1xAfH8+KFSuuqqt8+fL2OVi+vr40btyYESNGsHHjRnbv3s0PP/wASAjWrl2b4cOHs379evz8/Ejxov9h3fLyfZb69SEiAg4fhpUroW5dsytSZoiNjWXmzJm0bt0ai8XCkCFDnLZnc/jwYTZs2JBtW1RUFP3796d69eq8+uqrdO7cmVWrVjF+/Hjef/99AObOncvOnTupV68eERERzJ8/H5vNRlxcHL/88gtLliyhadOmFC1alF9++YXDhw8THx/vlJ/BFbn1HlGBAnDvvXJfr555r3feeYeIiAhq1apF69atadasGVWqVHHKZ33xxRdUrlw5223y5MlUqVKFr776ii+//JI77riDoUOH8sorr9CjRw9A+vbMnDmThg0bEh8fz8SJE5k2bRoJCQmEhoby008/0bJlS8qXL8/gwYN5++23adGihVN+BldkMQzzrxEeP36csLAw0tPTc72Kx8yZ0KEDlCkDO3Y49xDA3Zw5c4Zdu3ZRpkwZCmrPFOUEN/ody83ftVvvEQE0ayZ9iXbtgk2bzK5GKZUXbh9EQUHQtKnc96Jze0p5FLcPIrg0CVbPEynlntwziPbvh/R0+8N775VxRRs2yCGaUsq9uF8Q/fe/ULo0vPeefVORInBx2g6zZ5tTllIq79wviGJi4Px5GDsWzpyxb9ZR1kq5L/cLok6d4Lbb4O+/4dNP7Zuzgujnn2WAo1LKfbhfEBUoAM88I/dHjbLP7YiJgcqVZamhb781sT6lVK65XxAB9Oolczu2bcvWuFpbyKosV7Z1zUnPaovFwiwHXHp11Pt4E/cMouBgeOIJuT9ihL3rVtbh2eLFMuNbuZ/WrVvTvHnzaz63fPlyLBYLGzduzPX7OqJn9ZWGDRvGnXfeedX2AwcOOH16xtSpUwn3oB7J7hlEAP36gb+/rCt0cdbzHXdAuXJw9ix8953J9ak86dmzJ4sXL+bPP/+86rkpU6ZQrVq1bH2EcioyMjLfVrstXrw4/v7++fJZnsJ9g6hYMejeXe6PGAHIPDNtIeve7r33XiIjI+0tVrNkZGQwY8YMevbsyb///suDDz5IyZIlCQwMJDExkWnTpt3wfa88NNu+fTv16tWjYMGCVKxYkcWLF1/1PS+88ALly5cnMDCQsmXLMmTIEPuKplOnTmX48OGkpqbaVxjJqvnKQ7NNmzbRsGFDez/q3r17k3HZLnuPHj1o27Yto0aNIioqisKFC9OnT59bWj117969tGnThuDgYEJDQ+nUqRN///23/fnU1FQaNGhASEgIoaGhVK1alV9//RWAPXv20Lp1ayIiIggKCiIhIYH5Tm4O775BBNC/v6TPt9/Cli3ApfNEc+fCTfpiKRfk6+tLt27dmDp1araezTNmzCAzM5MHH3yQM2fOULVqVebNm8fvv/9O79696dq1K2vWrMnRZ9hsNtq3b4+fnx+//PILEydO5IUXXrjqdSEhIUydOpUtW7bw7rvvMnnyZEaPHg1A586d6d+/PwkJCRw4cIADBw7QuXPnq97j5MmTNGvWjIiICNauXcuMGTP4/vvv6du3b7bXLV26lB07drB06VI+/vhjpk6delUY55TNZqNNmzYcOXKEZcuWsXjxYnbu3JmtvuTkZEqVKsXatWv57bffGDhwoL1RXJ8+fTh79iw//fQTmzZt4q233sq2eopTOLyJbR7kpWe1Xfv20hT54YcNwzCMCxcMo2hR2bRokYMLdTNX9RO22aR5tBk3my3HdaelpRmAsXTpUvu2unXrGl26dLnu97Rq1cro37+//fGV/aUv71m9cOFCw9fX1/jrr7/szy9YsMAAjJSUlOt+xsiRI42qVavaH7/88stGUlLSVa+7/H0mTZpkREREGBmXNe2eN2+eYbVajYMHDxqGYRjdu3c3YmJijAsXLthfc//99xudO3e+bi1X9vO+3KJFiwwfHx9j79699m2bN282AGPNmjWGYRhGSEiIMXXq1Gt+f2JiojFs2LDrfvblvL5ntd1zz8nXzz6D/fvx8bm0LLVePbvCqVNyot+M28X+zDlRoUIFatWqxUcffQTA//73P5YvX07Pnj0ByMzM5NVXXyUxMZFChQoRHBzMwoULb9qjOktaWhrR0dGUKFHCvq1mzZpXvW769OnUrl2b4sWLExwczODBg3P8GZd/VlJSUrb2trVr18Zms2Vbry0hIcHeyRGk2dqt9M+Ojo4mOjravq1ixYqEh4eTlpYGwLPPPkuvXr1o3Lgxb775Jjt27LC/9sknn+S1116jdu3avPzyy3m6OJBb7h9Ed98trRnPn4d33wUunSeaPVvGFSn307NnT7755htOnDjBlClTKFeuHPXr1wdg5MiRvPvuu7zwwgssXbqUDRs20KxZs5v2qM6NVatWkZycTMuWLZk7dy7r169n0KBBDv2My+Vn/2yQK36bN2+mVatW/PDDD1SsWNHemrZXr17s3LmTrl27smnTJqpVq8a4ceOcVgt4QhABPP+8fJ04EY4fp1Ej+U94/35Yu9bc0lyKG3XP79SpE1arlS+++IJPPvmERx55BMvFrncrVqygTZs2dOnShaSkJMqWLcu2bdty/N7x8fHs27cv26qyq1evzvaalStXEhMTw6BBg6hWrRqxsbHs2bMn22v8/PzIvEmz9Pj4eFJTU7MtlrhixQqsVitxcXE5rjk3sn6+ffv22bdt2bKFY8eOUbFiRfu28uXL88wzz7Bo0SLat2/PlClT7M9FR0fz2GOPMXPmTPr378/kyZOdUmsWzwiili2hYkU4fhwmTcLfXzaBXj3Lxo265wcHB9O5c2defPFFDhw4YG+5CtKjevHixaxcuZK0tDQeffTRbFeEbqZx48aUL1+e7t27k5qayvLlyxk0aFC218TGxrJ3716+/PJLduzYwdixY69qZl+6dGl27drFhg0b+Oeffzh79uxVn5WcnEzBggXp3r07v//+O0uXLqVfv3507dqVYsWK5erf5EqZmZls2LAh2y0tLY3GjRuTmJhIcnIy69atY82aNXTr1o369etTrVo1Tp8+Td++ffnxxx/Zs2cPK1asYO3atfYe2U8//TQLFy5k165drFu3jqVLlzq9f7ZnBJHVCgMGyP3Ro+HcOR1l7QF69uzJ0aNHadasWbbzOYMHD6ZKlSo0a9aMe+65h+LFi9M263g8B6xWKykpKZw+fZoaNWrQq1cvXn/99Wyvue+++3jmmWfo27cvd955JytXrmTIkCHZXtOhQweaN29OgwYNiIyMvOYQgsDAQBYuXMiRI0eoXr06HTt2pFGjRowfPz53/xjXkJGRcVX/7KwFBGbPnk1ERAT16tWjcePGlC1blunTpwPg4+PDv//+S7du3ShfvjydOnWiRYsWDB8+HJCA69OnD/Hx8TRv3pzy5cvbFwFwFrfvWW139iyULSvHY1OmkN6uB5GRcuooLQ0qVHBsze5Ae1YrZ9Oe1Vfy94esuUUjRxIWYqNRI3moe0VKuTbPCSKA3r0hNFQGNy5YoKOslXITnhVEYWHw6KNyf8QI2rSRc6Rr1sBff5lbmlLq+jwriACeekp6Fv30E8V3r+buu2WztpBVynV5XhCVLAldusj9kSP16plSbsDzggguXcpPSeH+O7cD8OOPcPSoeSWZyQUujCoP5ajfLc8MoooVZY0hw6D0N2+TkAAXLsC8eWYXlr+y5i45a1qCUqcuziG8copKbvk6ohiX9Pzz0gtk6lSSHxvOS5uLMWvWpaM2b+Dr60tgYCCHDx+mQIECWK2e+f+Oyn+GYXDq1CkOHTpEeHh4tgm7eeG5QVSnjkyIXb2aHifG8RKvsWABnD4NAQFmF5c/LBYLUVFR7Nq166p5Uko5Qnh4OMWLF7/l9/GckdXXMnMmdOiAERFBfOBetv4VzJw50Lq14z7CHdhsNj08Uw5XoECBG+4J5ebv2nP3iEAaE8XGYtm+ndfv+JCOfz1FSor3BZHVatUpHsqlefZJAx8f+xW0VlvfwZfzzJkjJ66VUq7Ds4MIoFs3KFqUgof28nDQDP79177oh1LKRXh+EBUsCE8+CcBgvxGAoXPPlHIxnh9EAI8/DkFB3HY0lSYsJiXFviajUsoFeEcQFSoE//kPAAOtI9izBzZsMLckpdQl3hFEIL2KfHxoaFtCZdbp4ZlSLsR7gigmBh54AIDnGKmTYJVyId4TRGBfA60TX3Fi0y4uW8pJKWUi7wqipCRo1gwfbDzLO3p4ppSL8K4gAvteUU8+ZOmMf0wuRikF3hhEDRty7o4qBHKaqr+8Ty6Ww1JKOYn3BZHFgt9gWRm2L+OY/3XO12RXSjmH9wURQIcOHI0oQyT/cGrCx2ZXo5TX884g8vXlzOPPAtBi8yhOHLvx+uVKKefyziACir/4MEethSnLTjYNn2l2OUp5Na8NIktwEL/e3ReAYp+M0MlnSpnIa4MIIHxQH04RQLkjv3L++2Vml6OU1/LqIKraPJLpgQ8DcGzQCJOrUcp7eXUQWa2w475nycRK5NoFsHGj2SUp5ZW8OogA6nQvx9d0BMAYOcrkapTyTl4fRA0bwoQgmfZhTJsG+/aZXJFS3sfrg8jPD6JaV+MHGmDNvABjxphdklJex+uDCKBdOxiBTPswJk2Co0dNrkgp76JBBDRvDksLNGMjiVgyMmDiRLNLUsqraBABoaHQuImFkci5IsaOhTNnzC1KKS+iQXRR27bwJQ9w0C8aDh6Ezz4zuySlvIYG0UX33QeZlgK8de4Z2TBqFNhs5hallJfQILqoWDGoVQs+oBdnAsJh61b49luzy1LKK2gQXaZdO8gghJlFH5cNI3Tah1L5QYPoMm3bytfn9j2J4ecHK1fCihWm1qSUN9Aguky5cpCYCPttxdles7ts1L0ipZxOg+gK7drJ17EF+oPFAnPmwB9/mFuUUh5Og+gKWYdnH62I48K9beTBKJ0Mq5QzaRBd4c47ZXXq06dhRS2Z9sGnn8L+/abWpZQn0yC6gsVyaa9oyh81oU4dOHdORlsrpZxCg+gass4TffstZD57cdrHxIlw/Lh5RSnlwTSIrqF2bShcGI4cgZ9C74UKFSA9HSZPNrs0pTySBtE1+PrKlA+AlNlWeO7iXtHo0XKYppRyKA2i68g6TzRrFhgPJUNUFPz1F0ybZmZZSnkkDaLraNIEAgOlc+y6zf7w1FPyxMiRugaaUg6mQXQdAQHSMA1kr4hHH4WQENi8GRYsMLM0pTyOBtENZF09S0kBwsMljECnfSjlYBpEN9CqlZy43rwZtm9HDs98fWHZMpkQq5RyCA2iG4iIgHvukfuzZgGlSkGXLrKhbVtITTWnMKU8jAbRTVx+9QyQeWdVq8Lhw5JSv/xiTmFKeRANoptoc3He66pV0sqawoVhyRJp53jsGDRuDD/9ZGaJSrk9DaKbKFUKqleXK/Zz5lzcGBYGixbJMrEZGXJ5beFCU+tUyp1pEOVAtqtnWYKCYO5cOaN9+rQMxbYfvymlckODKAeyzhMtWXLFvNeAAJg5Ezp2lKkfHTvqyGul8kCDKAfi4yEuDs6fh/nzr3jSz0/Cp1s3yMyE5GT48ENT6lTKXWkQ5dBVV88u5+sLU6bAY4/JyaRevbR/kVK5oEGUQ1nniebPh7Nnr/ECqxXefx/695fHTz0Fb76Zb/Up5c40iHKoenWZgH/iBPzww3VeZLHIpNiXX5bHL74IgwfrJFmlbkKDKIes1kuHZ9munl3JYoFhwy7NR3v9dXj2WQ0jpW5AgygXsoJo9mw5L31Dzz0H770n98eMkQmzN/0mpbxTnoJo3759/Pnnn/bHa9as4emnn2bSpEkOK8wV3XOPjGU8dAhWr87BNzzxhJzEtlqlzWz37nDhgrPLVMrt5CmIHnroIZYuXQrAwYMHadKkCWvWrGHQoEG88sorDi3Qlfj5wb33yv3hw3O4g9OjB3zxhVxZ+/xz6NTpOme7lfJeeQqi33//nRo1agDw1Vdfcccdd7By5Uo+//xzpk6d6sj6XM6LL8o4xsWL4bXXcvhNnTvLwEc/PznB1LatjMZWSgF5DKLz58/j7+8PwPfff899FzvNV6hQgQMHDjiuOheUkCArC4HsFS1alMNvbN0a5s2T/rPffQctW8olOKVU3oIoISGBiRMnsnz5chYvXkzziz1V9+/fT+HChR1aoCvq1k3GLBqGDKS+7HTZjTVuLJNjQ0Lgxx+haVM4etSZpSrlFvIURG+99Rb/93//xz333MODDz5IUlISAHPmzLEfsnm6sWNleep//pEjr/Pnc/iNderIQKRCheSMd8OG0ttIKS9mMYy8DXDJzMzk+PHjRERE2Lft3r2bwMBAihYtmqv3On78OGFhYaSnpxMaGpqXckyxY4f0SEtPl6FCb7+di2/etEn2kA4dgooV5aRTiRJOq1Wp/Jabv+s87RGdPn2as2fP2kNoz549jBkzhq1bt+Y6hNxZuXJydR7gnXfkfHSOJSZKQ7WSJWHLFqhXD/bscUqdSrm6PAVRmzZt+OSTTwA4duwYd911F2+//TZt27ZlwoQJDi3Q1bVrd2l62cMPw//+l4tvjouD5cuhTBnZvapb92KXfqW8S56CaN26ddStWxeAr7/+mmLFirFnzx4++eQTxnrhrPM33oDataVXUceOubwyX6aMhFGFCrKaY9268PvvTqtVKVeUpyA6deoUISEhACxatIj27dtjtVq5++672eOFhxcFCsD06RAZKQt7PPlkLt+gZElZoqhSJfj7bxnC/dtvzihVKZeUpyC6/fbbmTVrFvv27WPhwoU0bdoUgEOHDrnVyWZHKllSBlBbLPDBB5DrcZ1Fi8LSpVCjBvz7r1xNW7HCGaUq5XLyFERDhw5lwIABlC5dmho1alCzZk1A9o4qV67s0ALdSePGMvEeZJrZpk25fINCheTqWb16cpzXtKn0p1XKw+X58v3Bgwc5cOAASUlJWK2SZ2vWrCE0NJQKFSrk6r3c9fL9tdhsMmh64UKIjYVff4Vc/0inTslZ8EWLwN8fvvlGmvQr5UZy83ed5yDKkjULv1SpUnl+D08KIpBBjpUry4jr+++X80cWSy7f5OxZGSk5e7ZMmP3iC3kzpdyE08cR2Ww2XnnlFcLCwoiJiSEmJobw8HBeffVVbDZbnor2JEWKwFdfSX7MmAHjx+fhTfz95ZsfeEBahzzwAFwcMqGUxzHyYODAgUZkZKTx/vvvG6mpqUZqaqrx3nvvGZGRkcZLL72U6/dLT083ACM9PT0v5bis0aMNAwyjQAHDWL06j29y4YJhPPKIvBEYxoQJjixRKafJzd91noIoKirKmD179lXbZ82aZZQoUSLX7+epQWSzGUaHDpIf0dGG8c8/eXyjzEzD6NfvUhi9/bZD61TKGXLzd52nQ7MjR45c84R0hQoVOHLkyC3toXkSi0WWOLv9dhmr2LWrnMzONasV3n0XBg6Ux/37Q5s2sG2bQ+tVyix5CqKkpCTGX+PEx/jx46lUqdItF+VJwsLg66+hYEFYsEBGYeeJxSLf/Oab4OMDc+ZIc6RnntFWIsrt5emq2bJly2jVqhW33XabfQzRqlWr2LdvH/Pnz7dP/8gpT7tqdi1TpsAjj8jOzeLFMl4xz9LSYMCAS8vOFiokA5gee0yGeSvlApx+1ax+/fps27aNdu3acezYMY4dO0b79u3ZvHkzn376aZ6K9nQPPyw3mw0efBD277+FN4uPl26PCxfKXtGRIzKvpFIl2a5LFyk3c8vjiC6XmppKlSpVyMzlsjnesEcEMk6xZk3YuPFSf7Rb3oG5cEHmlAwdeqnBWpMm0pfkjjtuuWal8srpe0QqbwID5XxRSAj8/DMMGuSAN/X1lUOy7dtlLTU/Pzn2S0qS7YcOOeBDlHIuDaJ8FhsLH30k90eOlIHTDhEWJqvLbtkCHTrIMeD//Z984IgRuoSRcmkaRCbo2BGeekrud+8OO3c68M3LlZPdrmXLoEoVmTz7wgtyXunrr/X8kXJJuTpH1L59+xs+f+zYMZYtW6bniHLg3DmoX1/651epIh0/ChZ08IfYbPDpp/DSS5fOjtetC6NHS7NtpZzIaeeIwsLCbniLiYmhW7dut1S8t/Dzk/lohQvDunXw9NNO+BCrVXa5tm2Tk9kBAdINslo12f7XX074UKVyz6FXzfLKG/eIsixcCC1ayBHTp59Cly5O/LB9+2Tv6LPP5HFgIDz/vJzkDgx04gcrb6RXzdxIs2YwZIjcf/RR2LzZiR8WHS1p98svUKuWjCcYNgzKl5ft2jlBmUSDyAUMHSrdHU+dkhPZGRlO/sAaNWT8wPTpEBMjh2jdusHdd2t7WmUKDSIX4OMDn38u6yv+8Qf07p0PF7csFujUST7wjTdkcNPatTLSslMn2LXLyQUodYkGkYsoWlR2UHx8YNo0mDgxnz64YEGZ1b99O/znPxJQM2bI5f6BA+Xyv1JOpkHkQurUgbfekvtPPy07KPmmWDGYNAnWr5cZuWfPSjGxsTB5MuRySIZSuaFB5GKefRbatpVxRvffL/NZ81VSEnz/vQz5jo2VKSK9e8tgJ11RRDmJBpGLsVikZUjZsrBnjwz3yfeLWRYL3HefrDg7ejSEh8tM3caNZfvWrflckPJ0GkQuKDxcZmP4+8PcuTJVzBR+fnKM+L//Qb9+cgLr22+hYsVLyx3pJX/lABpELqpyZRg3Tu4PGgQ//mhiMYULw9ixsod0770SPrNmySCouDh4+20TjiGVJ9EgcmG9esnwHptNVhM6eNDkgipUkD2izZtlDyk0VPaWBgyQNbcffhjWrNGJtSrXNIhcmMUC778vTRj//ls6O164YHZVyKHZ2LEykXbSJLjzTjhzBqZOhbvugurVZdWAU6fMrlS5CQ0iFxcUJCtOBwfL4dnQoWZXdJmgIBl7tG4drFolu2/+/vDbb7I7V7KknGPSk9vqJjSI3EBcnHSDBRkEPW+eufVcxWKR6SEffyzrbI8cKZf9jh2TZZAqVIBGjSRRz583u1rlgjSI3ETnztC3r9zv2lWuprukIkXknNH27fDdd3K532qVBt0dO8rctmHDtAWJykaDyI2MGiXzVY8ela8TJrjweWGrVa6qzZ4t89YGDZJ5LAcOwPDhEkgdOsjgSZf9IVR+0SByI/7+cljWsqXMwHjiCflbdvn1FW+7DV57Tfohffkl1KsnU0ZmzpQVRypUgDFj3OAHUc6iQeRmihSRQY7vvCNLEaWkyEUrt+je4ecnx5jLlsmYpD59ZNb/tm2yYm3JktCzJ/z6q9mVqnymQeSGLBb5u121Cm6/Hfbulf7Xr73mRnNTExJg/HgZAjBxoiwOefq0LHFSvboce06ZokMAvIQGkRurWlWunHfpIgE0ZIgc6dzSKrL5LThYWlNu2CC7dV26yJ7T2rWyRnepUjITeNs2sytVTqRB5OZCQqTL68cfy7CepUtlAr3LXeK/GYtF2td++qkMAXjrLShTRs4bjR4tYxiaNJFjUZcY1akcSZvne5Bt22QqyPr18viZZ2Tckb+/uXXlWWamrC4wYYIka9avauHCl0ZwZ92KFjW3VnWV3PxdaxB5mLNnZT3Fd9+Vx1WqyIWq2Fhz67plu3fLdJIPPoDDh69+/rbbsgdT1aqy+q0yjQaRYu5c6NED/v1XTsNMmODkpYryy7lzssu3dq3c1qyRKSTX+jWOi7sUTDVqyOVFh69iqa5Hg0gBMng5OVmuloNMBXvvPQkmj3L8uMxvywqntWulq9yVfH0hMTH7nlNCgmxXDqdBpOwyM+H112Uws80mh2hffimHbB7t8OHswbR2rbS9vVJAgDR/ujycbr9dRoarW6JBpK6yfDk89JBckPLzk66PTz4pF6u8gmHIyO7Lg+nXX6+9SklYmCzLfXk4lSrlRf9YjqFBpK7pyBEZuDxrljxu3VrGDxYpYmpZ5rHZZHLu5eG0fr30VrpSsWKXQumOOyAqCooXl+26XPc1aRCp6zIMabbWv79cYStRQhZ3vOcesytzEefPSwfKy8Np06YbD1kPDZVQutktMtKrzkdpEKmbSk2VaV9bt8oRx+DB0nTNi/5Ocu70aRn5nRVMO3ZI394DB66993Q9FouEUU5CKzzc7Q8FNYhUjpw8KeeJPvpIHtepA198AdHR5tblNgwDTpyQULrZ7e+/c7fiiZ+fBFLWIeDlt6zLnllBdaOvOXlNbl8bGCiLcN6EBpHKlWnTZLrXiRMQESHB1Lat2VV5mMxMGdSVFUwHDlw/tI4dM7vaG7vttmsPj7iCBpHKtR07pDl/1jLXffpIIzYd/2eCM2dkD+paIXXggBwqZv3Z3uhrTl6Tl9cWLw5z5tz0x9AgUnly7pw0Uhw1Sh5XqiRjjuLjza1Luafc/F3rqC1l5+cnfe8XLJBzqhs3ynCajz7Sbq7KuTSI1FWaN5erao0bS1+ynj1lMGR6utmVKU+lQaSuKSpKOnC88YYsef/llzITYs0asytTnkiDSF2X1QoDB8r0kJgYWYyjdm05fNNDNeVIGkTqpmrWlPF8998vzRGff15m8p87Z3ZlylNoEKkcCQ+H6dNleoiPD3z2GbRooeeNlGNoEKkcs1jg8cela2twsCzeWqeOTGpX6lZoEKlca9YMfvpJTmj//rsse5+aanZVyp1pEKk8qVwZVq+WBof790PdurBokdlVKXelQaTy7Lbb4OefpYXIiRPQqhVMnWp2VcodaRCpWxIeDt99JwMeL1yAhx+WtrR6eV/lhgaRumX+/rIu4osvyuNhw2Q09vnzppal3IgGkXIIqxX++19Zxt5qlWXrW7W6dktopa6kQaQc6tFHpUNEYCAsXgz16smyRkrdiAaRcrhWrWQttaJF5bL+3XfLZX6lrkeDSDlFtWpyeT8uTpYwqlNHBkAqdS0aRMppypSBlSslhNLTpb3IZ5+ZXZVyRRpEyqkKFZJzRZ06yVW0rl1l5Vm9vK8up0GknK5gQWnQP2CAPB48WE5qX7hgbl3KdWgQqXxhtUofo3HjZPLs5MnQpg1kZJhdmXIFGkQqX/XtCykpEBAA8+dD/fqyOIXybhpEKt+1aQNLl0qD/nXr5PJ+WprZVSkzaRApU9x1F6xaBbffLmv11aolrUWUd9IgUqYpV07CqGZNWdy0SRNp0q+8jwaRMlWRIrBkCbRrJz2wH3xQm/N7Iw0iZbqAAJgxA556Sh4//7yc1M7MNLculX80iJRL8PGBMWPgnXfk8v7778te0smTZlem8oMGkXIpzzwDX30lPY6+/RYaNIBDh8yuSjmbBpFyOR07ynmjQoVg7Vo5mb1tm9lVKWfSIFIuqXZtuaJWtizs3ClhtGKF2VUpZ9EgUi6rfHkJo+rV4cgRaNQIvvnG7KqUM2gQKZdWtKiMwm7dGs6elWWv33xTVg1RnkODSLm8oCCZn/bEEzK+6MUXZfzRvffCRx/BP/+YXaG6VRpEyi34+MD48fDeexAbK4Mf582T1UKKFZOra2PH6vLX7spiGOaPYT1+/DhhYWGkp6cTGhpqdjnKxRkGbNkCM2fKntL69dmfr1YN2reXcUgVKphTo8rd37UGkXJ7u3ZJIKWkyJW1y3+j4+MvhVKVKjJYUuUPDSLltQ4elOWMZs6UsUiXd4G87TYJpPbtZXiAj495dXoDDSKlkBn98+ZJKH33HZw6dem5yEjpi9S+PTRsKCO5lWNpECl1hVOnYNEiOXybM0dCKktIiFyBa9cOWrSA4GDTyvQoGkRK3cD58/Djj5fOK13eqtbfH5o1k1Bq3RoKFzatTLenQaRUDtls8Msvcvg2c6ZMJ8ni4yM9tdu3h7ZtoWRJ08p0SxpESuWBYcCmTbKXNHMmbNyY/fm77pJQuv9+WTxS3ZgGkVIOsGPHpVBaterSdqsVunSBIUOk57a6ttz8XevIaqWuo1w5WRRy5Ur46y9p1taggRzOffKJDJZ85JHsh3MqbzSIlMqBEiXg8cfhhx/knFKLFtLKdsoUiIuDXr1g926zq3RfGkRK5VKNGrI45KpVcoXtwgX48EOZA/foo7B3r9kVuh8NIqXy6O67ZaDkihXQuLEE0qRJct7o8cd1Am5uaBApdYtq1YLFi2H5chmlff48TJwogdSnD/z5p9kVuj4NIqUcpE4dmd/2448y/ujcOTnBXa4cPPkk7N9vdoWuS4NIKQerX1/C6IcfoG5dCaRx4ySQnn46+0huJTSIlHKSBg1g2TI5bKtVC86cgXfflcGQ/fvD33+bXaHr0CBSyoksFjmR/fPPsHChnOA+c0YWkixTBp57Dg4fNrtK82kQKZUPLBZo2lQGRy5YIEMATp+GUaMkkAYO9O7e2xpESuUjiwWaN4fVq2HuXKhaVZbVfustCaSXXoJ//zW7yvynQaSUCSwWaNVKVrKdMwcqV4aMDHjjDQmkwYNlLTdvoUGklIksFul79NtvMsE2KUnWbHv9dQmkl1/O3sTNU2kQKeUCLBbpebRunaxmm5gIx4/DK69A6dIwfDikp5tdpfNoECnlQqxW6Xm0YQN89RUkJEgADRsmgfTaa9IJ4Px5kwt1MO1HpJQLs9lgxgzZI0pLy/5cWJiseBsZKV+vvF25PTxcgi6/aGM0pTxMZqbsIb3xBmzeLAGVW1ar9ODOaXAVKSLLfed1LTgNIqU8mM0GR4/KuKMrb4cPX3t7Xs8v+ftfHVDR0TBixM2/V4NIKZXNuXMyPimnwXX4sIwAv5aYmJw1gcvN37Vv7n8kpZS78fODqCi55YRhyFpw1wooPz/H16dBpJS6isUi54eCgmQPyNn08r1SynQaREop02kQKaVMp0GklDKdBpFSynQaREop02kQKaVMp0GklDKdSwxozJplcvz4cZMrUUo5Stbfc05mkblEEJ04cQKA6OhokytRSjnaiRMnCAsLu+FrXGLSq81mY//+/YSEhGDJa88BpZRLMQyDEydOUKJECaw3aYTkEkGklPJuerJaKWU6DSKllOk0iJRSptMgUkqZToNIKWU6DSKllOk0iJRSptMgUi7LYrEwa9Yss8tQ+UCDSF1Tjx49sFgsV92aN29udmnKA7nEXDPlmpo3b86UKVOybfP39zepGuXJdI9IXZe/vz/FixfPdouIiADksGnChAm0aNGCgIAAypYty9dff53t+zdt2kTDhg0JCAigcOHC9O7dm4yMjGyv+eijj0hISMDf35+oqCj69u2b7fl//vmHdu3aERgYSGxsLHPmzLE/d/ToUZKTk4mMjCQgIIDY2NirglO5Bw0ilWdDhgyhQ4cOpKamkpyczAMPPEBaWhoAJ0+epFmzZkRERLB27VpmzJjB999/ny1oJkyYQJ8+fejduzebNm1izpw53H777dk+Y/jw4XTq1ImNGzfSsmVLkpOTOXLkiP3zt2zZwoIFC0hLS2PChAkUKVIk//4BlOMYSl1D9+7dDR8fHyMoKCjb7fXXXzcMwzAA47HHHsv2PXfddZfx+OOPG4ZhGJMmTTIiIiKMjIwM+/Pz5s0zrFarcfDgQcMwDKNEiRLGoEGDrlsDYAwePNj+OCMjwwCMBQsWGIZhGK1btzYefvhhx/zAylR6jkhdV4MGDZgwYUK2bYUKFbLfr1mzZrbnatasyYYNGwBIS0sjKSmJoKAg+/O1a9fGZrOxdetWLBYL+/fvp1GjRjesoVKlSvb7QUFBhIaGcujQIQAef/xxOnTowLp162jatClt27alVq1aefpZlbk0iNR1BQUFXXWo5CgBAQE5el2BAgWyPbZYLNhsNgBatGjBnj17mD9/PosXL6ZRo0b06dOHUaNGObxe5Vx6jkjl2erVq696HB8fD0B8fDypqamcPHnS/vyKFSuwWq3ExcUREhJC6dKlWbJkyS3VEBkZSffu3fnss88YM2YMkyZNuqX3U+bQPSJ1XWfPnuXgwYPZtvn6+tpPCM+YMYNq1apRp04dPv/8c9asWcOHH34IQHJyMi+//DLdu3dn2LBhHD58mH79+tG1a1eKFSsGwLBhw3jssccoWrQoLVq04MSJE6xYsYJ+/frlqL6hQ4dStWpVEhISOHv2LHPnzrUHoXIzZp+kUq6pe/fuBnDVLS4uzjAMOZH83nvvGU2aNDH8/f2N0qVLG9OnT8/2Hhs3bjQaNGhgFCxY0ChUqJDxn//8xzhx4kS210ycONGIi4szChQoYERFRRn9+vWzPwcYKSkp2V4fFhZmTJkyxTAMw3j11VeN+Ph4IyAgwChUqJDRpk0bY+fOnY7/x1BOp61iVZ5YLBZSUlJo27at2aUoD6DniJRSptMgUkqZTk9WqzzRI3rlSLpHpJQynQaRUsp0GkRKKdNpECmlTKdBpJQynQaRUsp0GkRKKdNpECmlTKdBpJQy3f8DNU2AWbgiw2kAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(model, EPOCHS, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"fine_tuned_code_comment_model\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Define the path to your saved model\n",
    "model_path = r\"E:\\Prasad\\NLP\\testing\\fine_tuned_code_comment_model\"\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Comment: Just a simple check to see if the x , y pair actually fits into the array .\n"
     ]
    }
   ],
   "source": [
    "code_snippet = \"private boolean boundsSafe ( int x , int y ) { if ( ( y < NUM_ ) || ( y >= height ) || ( x < NUM_ ) || ( x >= width ) ) { return BOOL_ ; } return BOOL_ ; }\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(code_snippet, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Generate prediction\n",
    "output = model.generate(**inputs)\n",
    "\n",
    "# Decode output\n",
    "generated_comment = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Comment:\", generated_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.4681\n"
     ]
    }
   ],
   "source": [
    "def generate_comment(code_snippet):\n",
    "    \"\"\"Generate a comment for the given code snippet using the model.\"\"\"\n",
    "    inputs = tokenizer(code_snippet, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    output = model.generate(**inputs)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def calculate_bleu(df):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for the test DataFrame.\n",
    "    :param df: DataFrame containing 'code' and 'comment' columns\n",
    "    :return: BLEU score\n",
    "    \"\"\"\n",
    "    references = [[ref.split()] for ref in df[\"comment\"].tolist()]  # Convert references into tokenized lists\n",
    "    candidates = [generate_comment(code).split() for code in df[\"code\"].tolist()]  # Generate comments and tokenize\n",
    "    \n",
    "    bleu_score = corpus_bleu(references, candidates)\n",
    "    return bleu_score\n",
    "\n",
    "# Compute BLEU score\n",
    "bleu = calculate_bleu(df_tst)\n",
    "print(f\"BLEU Score: {bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textstat\n",
      "  Downloading textstat-0.7.5-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pyphen (from textstat)\n",
      "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting cmudict (from textstat)\n",
      "  Downloading cmudict-1.0.32-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\spras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from textstat) (65.5.0)\n",
      "Collecting importlib-metadata>=5 (from cmudict->textstat)\n",
      "  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting importlib-resources>=5 (from cmudict->textstat)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata>=5->cmudict->textstat)\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading textstat-0.7.5-py3-none-any.whl (105 kB)\n",
      "Downloading cmudict-1.0.32-py3-none-any.whl (939 kB)\n",
      "   ---------------------------------------- 0.0/939.4 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 262.1/939.4 kB ? eta -:--:--\n",
      "   ---------------------- ----------------- 524.3/939.4 kB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 786.4/939.4 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 939.4/939.4 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.8/2.1 MB 1.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.0/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.6/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Installing collected packages: zipp, pyphen, importlib-resources, importlib-metadata, cmudict, textstat\n",
      "Successfully installed cmudict-1.0.32 importlib-metadata-8.6.1 importlib-resources-6.5.2 pyphen-0.17.2 textstat-0.7.5 zipp-3.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Flesch Reading Ease Score: 56.67\n",
      "Average Flesch-Kincaid Grade Level: 7.59\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "\n",
    "def calculate_readability(df):\n",
    "    \"\"\"\n",
    "    Calculate readability scores for the generated comments.\n",
    "    :param df: DataFrame containing 'code' and 'comment' columns\n",
    "    :return: Average readability scores (Flesch Reading Ease, Flesch-Kincaid Grade Level)\n",
    "    \"\"\"\n",
    "    generated_comments = [generate_comment(code) for code in df[\"code\"].tolist()]\n",
    "\n",
    "    # Compute readability scores\n",
    "    flesch_scores = [textstat.flesch_reading_ease(comment) for comment in generated_comments]\n",
    "    flesch_kincaid_scores = [textstat.flesch_kincaid_grade(comment) for comment in generated_comments]\n",
    "\n",
    "    # Average scores\n",
    "    avg_flesch_score = sum(flesch_scores) / len(flesch_scores)\n",
    "    avg_flesch_kincaid = sum(flesch_kincaid_scores) / len(flesch_kincaid_scores)\n",
    "\n",
    "    return avg_flesch_score, avg_flesch_kincaid\n",
    "\n",
    "# Compute Readability Score\n",
    "avg_flesch, avg_flesch_kincaid = calculate_readability(df_tst)\n",
    "\n",
    "print(f\"Average Flesch Reading Ease Score: {avg_flesch:.2f}\")\n",
    "print(f\"Average Flesch-Kincaid Grade Level: {avg_flesch_kincaid:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
